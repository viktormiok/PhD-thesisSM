\documentclass[a4paper]{article}
% \usepackage{paralist, natbib, graphicx, graphics, psfig, epsfig, amsmath,amssymb,amsthm,amsfonts, url, bm, amsthm, afterpage}

\usepackage{bm, graphicx, graphics, psfig, epsfig, amsmath, amssymb, amsthm, amsfonts, url, natbib, paralist, afterpage}
\usepackage{amssymb,amsfonts, bm}


% \usepackage{paralist, natbib, epsfig,graphicx,graphics,latexsym, psfig, epsfig, amsmath,amssymb,amsthm,amsfonts, url, bm, amsthm, afterpage}

\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}


% \newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
% \def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

\newtheorem{defin}{Definition}
\newtheorem{ther}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{prop}{Proposition}
\newtheorem{coro}{Corollary}
\newtheorem*{prop1}{Proposition 1}
\newtheorem*{prop2}{Proposition 2}
\newtheorem*{coro1}{Corollary 1}

\newtheorem{example}{Example}


\newcommand{\ttheta}{\mbox{\boldmath $\theta$}}
\newcommand{\aalpha}{\mbox{\boldmath $\alpha$}}

\setlength{\evensidemargin}{.5cm}
\setlength{\oddsidemargin}{.5cm}
\setlength{\textwidth}{15cm}
\setlength{\plitemsep}{2pt}
\setlength{\pltopsep}{2pt}


\def\fat#1{\mbox{\boldmath$#1$}}
\def\reminder#1{\marginpar{\rule[0pt]{1mm}{11pt}}\textbf{#1}}

\bmdefine\ttheta{\theta}
\bmdefine\aalpha{\alpha}
\bmdefine\bbeta{\beta}
\bmdefine\ddelta{\delta}
\bmdefine\llambda{\lambda}
\bmdefine\ggamma{\gamma}
\bmdefine\nnu{\nu}
\bmdefine\vvarepsilon{\varepsilon}
\bmdefine\mmu{\mu}
\bmdefine\nnu{\nu}
\bmdefine\ttau{\tau}
\bmdefine\SSigma{\Sigma}
\bmdefine\TTheta{\Theta}
\bmdefine\XXi{\Xi}
\bmdefine\PPi{\Pi}
\bmdefine\GGamma{\Gamma}
\bmdefine\DDelta{\Delta}
\bmdefine\ssigma{\sigma}
\bmdefine\UUpsilon{\Upsilon}
\bmdefine\PPsi{\Psi}
\bmdefine\PPhi{\Phi}
\bmdefine\LLambda{\Lambda}
\bmdefine\OOmega{\Omega}


\title{Notes on multivariate modeling of gene expression data}
\author{ {\small
\textbf{Wessel N. van Wieringen}$^{1,2,}$ }
\\
{\small $^1$ Department of Epidemiology and Biostatistics, VU University Medical Center}
\\
{\small P.O. Box 7075, 1007 MB Amsterdam, The Netherlands}
\\
{\small $^2$ Department of Mathematics, VU University Amsterdam}
\\
{\small De Boelelaan 1081a, 1081 HV Amsterdam, The Netherlands}
\\
{\small Email: w.vanwieringen@vumc.nl}
}
\date{}



\begin{document}
\maketitle


\begin{abstract}
\noindent
Notes on the application of time series machinery to the multivariate analysis of gene expression data from a time-course experiment. The intend is to include time-varying covariates, i.e. data from other molecular levels, in the model and analysis. These notes are not meant for publication.
\\
\\
\textbf{Acknowledgment:} Carel F.W. Peeters and Mark A. van de Wiel provided feedback on earlier versions of this document that lead to this improved version.
\\
\\
\textbf{Disclaimer:} This document is incomplete and does most likely contain errors, which are the full responsibility of the author.
\end{abstract}

\newpage
\tableofcontents

\newpage
\section{Some necessary biology}
\subsection{Molecular biology}
All organisms are made of cells. A cell is the smallest independently living unit. The cell harbors many molecular entities, each fulfilling a specific role in the functioning of the cell. The molecular entities come in many different types (e.g., DNA, mRNA, et cetera) here referred to as molecular levels. Some of the key players involved in the regulatory mechanism of the cell are introduced.

The DNA molecule is a double-stranded polymer composed of four nucleotides. The nucleus of human cells contains DNA molecules in the form of chromosomes, 22 autosomal and one sex chromosome pair. Hence, any part of a DNA molecule belonging to one of the autosomal chromosomes is present twice in a normal human cell. Together the haploid (single) set of chromosomes make up the complete genetic constitution of an organism, the genome. The DNA molecules carry, in the sequence of nucleotides, the information necessary for the functioning of cells, which is encoded in molecular units called genes. A gene is the basic physical unit of heredity. A gene is said to be expressed if the product that it encodes for has been formed. The central dogma of molecular biology describes the information transfer process that leads from the information encoded in DNA (genes) to the proteins of the cell. Three primary steps in this information processes are discerned: replication, transcription, and translation. Replication refers to the process of DNA duplication. Transcription is the process in which messenger RNA (mRNA) molecules are synthesized (copying of genetic information) from the DNA in the cell nucleus. Much like DNA, an mRNA molecule is a chain of nucleotides, but single stranded and consisting of a different set of nucleotides. The sequence of nucleotides, arranged into trios (codons), of an mRNA encodes for a sequence of amino acids (each codon corresponds to a specific amino acid) that form a protein. After transcription, the mRNA molecules are transported from the nucleus to the cytoplasm, where they are converted to proteins (translation). A protein is a molecule consisting of a sequence of amino acids, whose order is determined by the base sequence of nucleotides in the mRNA (gene) encoding for the protein. Proteins are important molecules for the functioning of the cell as they participate in many of its processes. Proteins are not considered in the remainder.



\subsection{Cancer}
Cancer is a disease in which cells exhibit unproliferated growth and can invade nearby tissues (so-called cancer cells). Cancer is a genetic disease, often caused by abnormalities in the genetic material of cancer cells. An example of such abnormalities are chromosomal aberrations, a structural abnormality in the chromosomes. This can be an abnormal DNA copy number, in which case the number of copies of a particular genomic segment deviates from the usual two (assuming the segment maps to an autosomal chromosome). Such chromosomal aberrations are a key event in the development and progression of cancer \cite{Len1998}. Chromosomal aberrations like an increase (or decrease) of the DNA copy number of a particular genomic segment are, through the central dogma of biology, likely to result in increased (or decreased) mRNA transcription levels of genes in the segment.

Among others aforementioned abnormalities affect so-called cancer genes. Cancer genes have the ability to direct malignant cell growth. Two types of cancer genes are distinguished: proto-onco genes and tumor-suppressor genes \cite{Vog2004}. Proto-onco genes often have a gain (more than two copies) in DNA copy number, which causes increased proliferation. Tumor-suppressor genes often have a loss (less than two copies) in DNA copy number, which causes impairment of growth inhibition. For more on cancer refer to \cite{Wein2006}.


\subsection{Measurement devices}
High-throughput techniques are used to quantify the type and amount of information on the different molecular levels of the cell. For instance, DNA copy numbers can be measured by array CGH \cite{Pin2005}, and mRNA levels by gene expression microarrays \cite{Ngu2002}. Both are a type of microarray. Conceptually, a microarray is a large collection of gauges. Each gauge measures a characteristic (e.g., DNA copy number, gene expression) of a pre-defined molecular entity (e.g. genomic segment, transcript). The result of measuring a sample's gene expression by a microarray is a vector of (say) 10000 values on a continuous scale. These values are preprocessed to remove experimental artifacts and arrive at the quantity of interest (refer to \cite{Ngu2002} for more details on the preprocessing of gene expression data). The preprocessed values are believed to be representative (proportionally) to the expression levels of the corresponding genes. The vector of preprocessed values is referred to as the sample's gene expression profile.

Using array CGH, one obtains a data vector of similar size (although current platforms produce much larger vectors). After preprocessing, the values now represent the {\it relative} DNA copy number (on a continuous scale). Relative, for the DNA copy number of the sample of interest is measured in comparison to a reference sample that is believed to have a DNA copy number of two for the autosomale chromosomes. Often the array CGH data undergo further processing steps: segmentation and calling. Segmentation divides the genome into non-overlapping segments separated by breakpoints. These breakpoints indicate a change in DNA copy number, and, hence, DNA copy number is constant in between breakpoints. Segmentation may be viewed as a piece-wise constant smoothing of the data. As a final and last pre-processing step, referred to as calling, the DNA copy number of each segment is determined. At present calling algorithms cannot determine whether there are, say, three or four copies present. They can however detect deviations from the normal copy number, and classify each segment as either `normal', `loss', `gain'  or `amplification': `normal' if there are two copies of the chromosomal segment present, `loss' (also named deletion) if at least one copy is lost, `gain' if at least one additional copy is present, and `amplification' if there are high level, say $> 5$, copy numbers (for simplicity we ignore the existence of polyploid genomes). These labels are referred to as calls.
\\
\\
Add microRNAs. Sequencing data. Pathways.




\newpage
\section{Graph theory}
Some terminology from graph theory is introduced. The definitions below are taken from several sources, among others Cowell, Dawid, Lauritzen, Spiegelhalter (1999), Shipley (2000), and Huber \textit{et al}. (2007).

A \textit{graph} is a system of interconnected components. The topology of a graph (also referred to as a \textit{network} when stressing the application rather than the mathematical concept) is defined as a pair $(\mathcal{V}, \mathcal{E})$. The $\mathcal{V}$ is a finite set of \textit{nodes}, also called \textit{vertices} of graph components.
The $\mathcal{E}$ is the subset of $\mathcal{V} \times \mathcal{V}$ of ordered pairs of nodes, called \textit{edges} or \textit{links} of the graph. The \textit{degree} of a node is the number of edges incident to the node.
The nodes are the entities of interest and the edges represent the relationships between the entities. The relationship modeled by the edges may be binary (the edge is absent or present) or continuous, in which case the value is called a weight.

There is an edge between $V_{1}$ and $V_{2}$ if and only if there is an edge from $V_{1}$ to $V_{2}$ or an edge from $V_{2}$ to $V_{1}$.
If both ordered pairs $(V_1, V_2)$ and $(V_2, V_1)$ belong to $\mathcal{E}$, the graph has an \textit{undirected edge} between $V_1$ and $V_2$, and $V_1$ and $V_2$ are called \textit{neighbors}. If $(V_1, V_2) \in \mathcal{E}$ but $(V_1, V_2) \notin \mathcal{E}$,
the edge is \textit{directed} pointing from $V_1$ to $V_2$ ($V_1 \rightarrow V_2$), and $V_1$ is called a \textit{parent} of $V_2$, and, similarly, $V_2$ is called a \textit{child} of $V_1$. If $(V_1, V_1) \in \mathcal{E}$, the graph contains a \textit{self-loop}.
If an edge (pointing in either direction) connects $V_1$ and $V_2$, these nodes are called \textit{adjacent}.
The \textit{adjacency matrix} $A$ of a graph is a square matrix $A$ with elements $a_{ij}$ whose rows and columns correspond to nodes and whose elements denote the presences (or weights) of the edges.

A \textit{(un)directed graph} comprises of (un)directed edges only.
A directed graph is sometimes refered to as \textit{digraph}.
 A \textit{trail} in a directed graph is an ordered $(n+1)$-tuple of
nodes $V_1, \ldots,V_{n+1}$ where for $1 \leq i \leq n$, either $(V_{i+1}, V_{i})$ or $(V_i, V_{i+1})$ is an edge in the graph, and each edge occurs at most once.
A \textit{path} is defined similarly, but with no repeated nodes, except possibly the first and the last, in which case it is a called a \textit{cycle}.
A \textit{directed path of length $n$} in a directed graph $(\mathcal{V}, \mathcal{E})$ is an ordered $(n+1)$-tuple of vertices $(V_1, \ldots, V_{n+l})$ where for $1 \leq i \leq n$, $(V_i, V_{i+l})$ is an edge in the graph.
If and only if there is a directed path from $V_{1}$ to $V_{2}$, $V_{1}$ is called an \textit{ancestor} of $V_{2}$ and $V_2$ is called a \textit{descendant} of $V_1$. Since the empty path from $V_1$ consists simply of $V_1$, every node is a descendant of itself. The \textit{distance} between two nodes in a graph is the number of nodes in a shortest path connecting them.

A \textit{complete graph} contains all possible edges, i.e., all pairs of nodes are neighbors.
A \textit{clique} is a subgraph of $(\mathcal{V}, \mathcal{E})$ that is complete, i.e., every node in the clique is connected to every other node in the clique. Note that a graph $\mathcal{G}_A = (A, E_A)$ is a \textit{subgraph} of $\mathcal{G} = (V, E)$ if $A \subseteq V$ and $V_A \subseteq (A \times A)$. The \textit{clustering coefficient} of a node in a graph quantifies how close the node and its neighbors are from being a clique. The clustering coefficient of the network may be defined as the average clustering coefficients of all nodes in the network.

An \textit{$n$-cycle} is a path of length $n$ with the modification that the end points are identical. Similarly, a \textit{directed $n$-cycle} is a directed path with coinciding start and end point. An \textit{acyclic} graph does not possess cycles.
A directed graph $\mathcal{G}$ which is acyclic is called a \textit{directed acyclic graph}, in short DAG.
A directed graph is acyclic if and only if on every directed path in the graph no node occurs more than once.

Other important graphs are:
\begin{itemize}
\item In a \textit{regular graph} all nodes have the same degree.
\item A \textit{complete graph} is a graph such that every pair of nodes is joined by an edge.
\item A \textit{random graph} is a graph that is generated by a random process: start with $n$ nodes and add edges between at random.
\item In a \textit{scale-free graph} the distribution of a node's degree $k$ follows a power law distribution: $P(K=k) \sim k^{-\gamma}$. In a scale-free graph some nodes are highly connected (called hubs), and most are of low degree. The graph is called `scale-free' because the properties are independent of its size.
\item A \textit{small-world graph} has a small mean-shortest path length and an (average) clustering coefficient that is significantly higher than that of a random graph constructed on the same set of nodes. Small-world graphs will have a high representation of cliques.
\item \textit{Bipartite graphs} $\mathcal{G} = (\mathcal{V}, \mathcal{E})$ can be partitioned into two sets $\mathcal{U}, \mathcal{W} \subset \mathcal{V}$ such that every edge in $\mathcal{E}$ is an undirected relationship between one node in $\mathcal{U}$ and one node in $\mathcal{W}$, and there are no edges between elements of $\mathcal{U}$ and between elements of $\mathcal{W}$.
\end{itemize}

A sequence of nodes constituting an undirected path from $V_1$ to $V_2$ has a \textit{collider} at $W$ if there are two edges along the path pointing into $W$. When two nodes with edges pointing into a collider are not adjacent, the collider is called \textit{unshielded}.

Let $\mathcal{G} = (\mathcal{V}, \mathcal{E})$ with $\mathcal{V} = \{V_1, V_2, V_3, V_4\}$ and
$\mathcal{E} = \{V_1 \rightarrow V_3, V_2 \rightarrow V_3, V_3 \rightarrow V_4\}$, as depicted in Figure
\ref{fig.toy.network.1}.
Clearly, this is a DAG. In this DAG, $V_1$ and $V_3$ are neighbors, whereas $V_1$ and $V_2$ are not.
It contains directed paths from $V_1$ to $V_4$ and $V_2$ to $V_4$, making, e.g., $V_1$ a parent of $V_4$, $V_3$ a child of $V_2$,
and $V_4$ a descendant of $V_1$. It also contains an undirected path between $V_1$ and $V_2$ with a
collider at $V_3$, which is unshielded.

\begin{figure}[ht]
\begin{center}
\begin{minipage}[m]{4.5cm}
\begin{center}
\includegraphics[angle=270, scale=0.225]{network1.ps}
\end{center}
\begin{center}
\caption{Toy network 1. \label{fig.toy.network.1}}
\end{center}
\end{minipage}
\begin{minipage}[m]{4.5cm}
\begin{center}
\includegraphics[angle=270, scale=0.225]{network2.ps}
\end{center}
\begin{center}
\caption{Toy network 2. \label{fig.toy.network.2}}
\end{center}
\end{minipage}
\begin{minipage}[m]{4.5cm}
\begin{center}
\includegraphics[angle=270, scale=0.225]{network3.ps}
\end{center}
\begin{center}
\caption{Toy network 3. \label{fig.toy.network.3}}
\end{center}
\end{minipage}
\end{center}
\end{figure}

Consider a DAG $\mathcal{G}$ with nodes $V_1, \ldots, V_K$. Then, a distribution $F$ for $V_1, \ldots, V_K$ is \textit{Markov} to $\mathcal{G}$ or
$\mathcal{G}$ represents $F$ if and only if:
\begin{eqnarray*}
P_{X_1, \ldots, X_K}(x_1, \ldots, x_K) & = & \prod_{i = 1}^K P_{X_i \, | \, X_{\mbox{pa}_i}} (x_i \, | \,  x_{\mbox{pa}_i}),
\end{eqnarray*}
where $\mbox{pa}_i$ is the set of parents of node $V_i$ in $\mathcal{G}$. The set of distributions $F$ that are represented by $\mathcal{G}$ is denoted $M(\mathcal{G})$.

If $\mathcal{G}$, as given in Figure \ref{fig.toy.network.1}, represents the joint distribution on $(X_1, X_2, X_3, X_4)$, the random variables corresponding to
$\{V_1, V_2, V_3, V_4\}$, we have
\begin{eqnarray*}
P (X_1, X_2, X_3, X_4) & = & P(X_1) \, P(X_2) \, P(X_3 \, | \, X_1, X_2) \, P(X_4 \, | \, X_3).
\end{eqnarray*}
Note that as, $V_1$ and $V_2$ have no parents, $P(X_1)$ and $P(X_2)$ may be viewed as priors.

Let $V_1$ and $V_2$ be distinct nodes of a DAG and $Q$ be a set of nodes not containing $V_1$ and $V_2$. Then, $V_1$ and $V_2$ are \textit{d-connected (directed connected) given $Q$} if there is an undirected path $\mathcal{P}$ between $V_1$ and $V_2$ such that:
\begin{itemize}
\item every collider in $\mathcal{P}$ has a descendant in $Q$, and
\item no other node on $\mathcal{P}$ is in $Q$.
\end{itemize}
If $V_1$ and $V_2$ are not d-connected given $Q$, they are \textit{d-separated (directed separated) given $Q$}.
For clarity we give an alternative (but equivalent) definition of d-seperated.
A trail $\mathcal{P}$ from $V_1$ to $V_2$ in a DAG $\mathcal{G}$ is said to be \textit{blocked} by $Q$ is it contains a node $V_3 \in \mathcal{P}$ such that either:
\begin{itemize}
\item $V_3 \in Q$ and edges of $\mathcal{P}$ do not meet head-to-head (collide) at $V_3$, and
\item $V_3$ and all its decendants ar not in $Q$, and edges of $\mathcal{P}$ meet head-to-head at $V_3$.
\end{itemize}
A trail that is blocked by $Q$ is daid to be d-seperated by Q. Two subsets $\mathcal{U}$ and $\mathcal{W}$ of $\mathbf{V}$ are said to be d-seperated by Q if all trails from $\mathcal{U}$ to $\mathcal{W}$ are blocked by $Q$.

The notion of d-seperated is illustrated using Figures \ref{fig.toy.network.2} and \ref{fig.toy.network.3}.
In Figure \ref{fig.toy.network.2} $V_2$ and $V_3$ are d-seperated given $Q = \{V_1, V_4\}$, similarly
$V_2$ and $V_5$ are d-seperated given $Q = \{V_1, V_4\}$, and, consequently,
$V_2$ and $\{V_3, V_5\}$ are d-seperated given $Q = \{V_1, V_4\}$.
Make similar statements about Figure 3 (see Shipley, 2000).

If can be shown (see Cowell, Dawid, Lauritzen, Spiegelhalter, 1999) that for any DAG $\mathcal{G}$
\begin{eqnarray*}
A \independent B \, | \, Q \qquad \Longleftrightarrow \qquad Q \mbox{ d-seperates } A \mbox{ from } B.
\end{eqnarray*}
Hence, any d-seperation statement in a DAG can be translated into a (conditional) independence statement.



\newpage
\section{The multivariate normal distribution}
A $p$-dimensional random variable $\mathbf{Y} = (Y_{1}, Y_2, \ldots,  Y_{p})^{\mathrm{T}}$ following a multivariately normal distribution, denoted $\mathbf{Y} \sim \mathcal{N}( \mmu, \SSigma)$, has density:
\begin{eqnarray*}
f_{\mathbf{Y}} (\mathbf{y}) & = &
(2 \pi)^{-p/2} | \SSigma |^{-1/2} \, \exp [ -\frac{1}{2} (\mathbf{y} - \mmu)^{\mathrm{T}} \SSigma^{-1} (\mathbf{y} - \mmu)].
\end{eqnarray*}
The density function of a multivariate normal random variable $\mathbf{Y}$ is often denoted by $\phi_{\mmu, \SSigma}(\mathbf{y})$. The multivariate normal distribution has two parameters: the mean $\mmu$ and the covariance $\SSigma$. The mean is defined as:
\begin{eqnarray*}
\mmu & = & (\mu_1, \ldots, \mu_p)^{\mathrm{T}}
\\
& = & (E(Y_1), \ldots, E(Y_p))^{\mathrm{T}}
\\
& = & E(\mathbf{Y})
\\
& = & \int_{\mathbb{R}^p} \mathbf{y} \, \phi_{\mmu, \SSigma}(\mathbf{y}) \, d \mathbf{y}.
\end{eqnarray*}
The covariance matrix is a $p \times p$ positive definite matrix defined as:
\begin{eqnarray*}
\SSigma
& = &
\left(
\begin{array}{cccc}
\sigma_{11} & \sigma_{12} & \ldots & \sigma_{1p}
\\
\sigma_{21} & \sigma_{22} & \ldots & \sigma_{2p}
\\
\vdots & \vdots & \ddots & \vdots
\\
\sigma_{p1} & \sigma_{p2} & \ldots & \sigma_{pp}
\end{array}
\right)
\\
& = &
\left(
\begin{array}{cccc}
\mbox{Cov}(Y_1, Y_1) & \mbox{Cov}(Y_1, Y_2) & \ldots & \mbox{Cov}(Y_1, Y_p)
\\
\mbox{Cov}(Y_2, Y_1) & \mbox{Cov}(Y_2, Y_2) & \ldots & \mbox{Cov}(Y_2, Y_2)
\\
\vdots & \vdots & \ddots & \vdots
\\
\mbox{Cov}(Y_1, Y_1) & \mbox{Cov}(Y_p, Y_2) & \ldots & \mbox{Cov}(Y_p, Y_p)
\end{array}
\right)
\\
& = & \mbox{Cov}(\mathbf{Y}, \mathbf{Y})
\\
& = & \int_{\mathbb{R}^p} (\mathbf{y} - \mmu) \, (\mathbf{y} - \mmu)^{\mathrm{T}} \, \phi_{\mmu, \SSigma}(\mathbf{y}) \, d \mathbf{y}.
\end{eqnarray*}
Hence, the elements of $\SSigma$ are given by:
\begin{eqnarray*}
( \SSigma )_{j_1, j_2} & = & \sigma_{j_1, j_2}
\\
& = & \mbox{Cov}(Y_{j_1}, Y_{j_2})
\\
& = & E\{ [ Y_{j_1} - E (Y_{j_1})] [ Y_{j_2} - E (Y_{j_2})] \}
\\
& = & E\{ [ Y_{j_1} - \mu_{j_1}] [ Y_{j_2} - \mu_{j_2}] \}
\\
& = & E\{  Y_{j_1} Y_{j_2} - \mu_{j_1} \mu_{j_2}] \}.
\end{eqnarray*}
Consequently, as $\sigma_{j_1, j_2} = \sigma_{j_2, j_1}$ the covariance matrix $\SSigma$ is symmetric.


The covariance matrix is often parametrized differently as:
\begin{eqnarray*}
\SSigma & = & \UUpsilon \mathbf{R} \UUpsilon
\\
& := &
\left(
\begin{array}{cccc}
\sigma_{1} & 0 & \ldots & 0
\\
0 & \sigma_{2} & \ldots & 0
\\
\vdots & \vdots & \ddots & \vdots
\\
0 & 0 & \ldots & \sigma_{p}
\end{array}
\right)
\left(
\begin{array}{cccc}
1 & \rho_{12} & \ldots & \rho_{1p}
\\
\rho_{21} & 1 & \ldots & \rho_{2p}
\\
\vdots & \vdots & \ddots & \vdots
\\
\rho_{p1} & \rho_{p2} & \ldots & 1
\end{array}
\right)
\left(
\begin{array}{cccc}
\sigma_{1} & 0 & \ldots & 0
\\
0 & \sigma_{2} & \ldots & 0
\\
\vdots & \vdots & \ddots & \vdots
\\
0 & 0 & \ldots & \sigma_{p}
\end{array}
\right)
\\
& = & \left(
\begin{array}{cccc}
\sigma_{1}^2 & \sigma_{1} \sigma_{2} \rho_{12} & \ldots & \sigma_{1} \sigma_{p} \rho_{1p}
\\
\sigma_{2} \sigma_{1} \rho_{21} & \sigma_{2}^2 & \ldots & \sigma_{2} \sigma_{p} \rho_{2p}
\\
\vdots & \vdots & \ddots & \vdots
\\
\sigma_{p} \sigma_{1} \rho_{p1} & \sigma_{p} \sigma_{2} \rho_{p2} & \ldots & \sigma_{p}^2
\end{array}
\right).
\end{eqnarray*}
The matrix $\mathbf{R}$ is called the correlation matrix for, as
\begin{eqnarray*}
\rho_{j_1, j_2} = \mbox{Cov}(Y_{j_1}, Y_{j_2}) \, / \, \sqrt{ \mbox{Var}(Y_{j_1}) \mbox{Var}(Y_{j_1}) },
\end{eqnarray*}
we have:
\begin{eqnarray*}
\mathbf{R} & = & \mbox{Corr}(\mathbf{Y}, \mathbf{Y)})
\\
& = &
\left(
\begin{array}{cccc}
\mbox{Corr}(Y_1, Y_1) & \mbox{Corr}(Y_1, Y_2) & \ldots & \mbox{Corr}(Y_1, Y_p)
\\
\mbox{Corr}(Y_2, Y_1) & \mbox{Corr}(Y_2, Y_2) & \ldots & \mbox{Corr}(Y_2, Y_2)
\\
\vdots & \vdots & \ddots & \vdots
\\
\mbox{Corr}(Y_1, Y_1) & \mbox{Corr}(Y_p, Y_2) & \ldots & \mbox{Corr}(Y_p, Y_p)
\end{array}
\right).
\end{eqnarray*}
Clearly, as $\sigma_j > 0$ for all $j=1, \ldots, p$, and thus $\UUpsilon$ is non-singular, one can always convert a covariance matrix to a correlation matrix by pre- and post-multiplication by $\UUpsilon^{-1}$:
\begin{eqnarray*}
\mathbf{R} & = & \UUpsilon^{-1} \SSigma \UUpsilon^{-1}
\\
& = & \left(
\begin{array}{cccc}
\sigma_{1}^{-1} & 0 & \ldots & 0
\\
0 & \sigma_{2}^{-1} & \ldots & 0
\\
\vdots & \vdots & \ddots & \vdots
\\
0 & 0 & \ldots & \sigma_{p}^{-1}
\end{array}
\right)
\left(
\begin{array}{cccc}
\sigma_{11} & \sigma_{12} & \ldots & \sigma_{1p}
\\
\sigma_{21} & \sigma_{22} & \ldots & \sigma_{2p}
\\
\vdots & \vdots & \ddots & \vdots
\\
\sigma_{p1} & \sigma_{p2} & \ldots & \sigma_{pp}
\end{array}
\right)
\left(
\begin{array}{cccc}
\sigma_{1}^{-1} & 0 & \ldots & 0
\\
0 & \sigma_{2}^{-1} & \ldots & 0
\\
\vdots & \vdots & \ddots & \vdots
\\
0 & 0 & \ldots & \sigma_{p}^{-1}
\end{array}
\right).
\end{eqnarray*}
One cannot recover $\SSigma$ from $\mathbf{R}$ without knowledge of $\UUpsilon$.

\begin{example}
A set of independent $p$ random  variables $Y_j$, distributed as $Y_j \sim \mathcal{N}( \mu_j, \sigma_j^2)$ for $j=1, \ldots, p$, can be written as a $p$-variate normal distribution:
\begin{eqnarray*}
\left(
\begin{array}{l}
Y_{1}
\\
Y_{2}
\\
\vdots
\\
Y_{p}
\end{array}
\right)
& \sim &
\mathcal{N} \left(
\left(
\begin{array}{l}
\mu_{1}
\\
\mu_{2}
\\
\vdots
\\
\mu_{p}
\end{array}
\right),
\left(
\begin{array}{cccc}
\sigma_{1}^{2} & 0 & \ldots & 0
\\
0 & \sigma_{2}^{2} & \ldots & 0
\\
\vdots & \vdots & \ddots & \vdots
\\
0 & 0 & \ldots & \sigma_{p}^{2}
\end{array}
\right)
\right).
\end{eqnarray*}
The independence of the $Y_j$ implies that $\mbox{Cov}(Y_{j_1}, Y_{j_2}) = 0$, and thus that all off-diagonal element of $\SSigma$ are zero.
\end{example}



\begin{example} \textit{(The bivariate normal distribution)}
\\
The special case $p=2$ is called the bivariate normal distribution. A 2-dimensional random variable is bivariate normally distributed, denoted:
\begin{eqnarray*}
\left(
\begin{array}{l}
Y_{1}
\\
Y_{2}
\end{array}
\right)
& \sim &
\mathcal{N} \left(
\left(
\begin{array}{l}
\mu_{1}
\\
\mu_{2}
\end{array}
\right),
\left(
\begin{array}{ll}
\sigma_{1}^{2} & \sigma_1 \sigma_2 \rho_{12}
\\
\sigma_1 \sigma_2 \rho_{12} & \sigma_{2}^{2}
\end{array}
\right)
\right),
\end{eqnarray*}
if its density is:
\begin{eqnarray*}
& & \hspace{-1cm} f_{(Y_1, Y_2)} (y_1, y_2) 
\\
& = &
(2 \pi)^{-1} [\sigma_1^2 \sigma_2^2 (1-\rho^2)]^{-1/2} 
\\
& & \times \, \, \, \exp \left\{ -\frac{1}{2}
[\sigma_1^2 \sigma_2^2 (1-\rho^2)]^{-1}
\, 
\left(
\begin{array}{l}
y_{1} - \mu_1
\\
y_{2} - \mu_2
\end{array}
\right)^{\mathrm{T}} 
\left(
\begin{array}{ll}
\sigma_{2}^{2} & -\sigma_1 \sigma_2 \rho_{12}
\\
-\sigma_1 \sigma_2 \rho_{12} & \sigma_{1}^{2}
\end{array}
\right)
\left(
\begin{array}{l}
y_{1} - \mu_1
\\
y_{2} - \mu_2
\end{array}
\right) \right\}.
\end{eqnarray*}
The density of the bivariate normal distribution is visualized in Figure \ref{fig.bivariateNormal} for several choices of the covariance parameters.


\begin{figure}[b!]
\centering
\begin{tabular}{ccc}
\psfig{file=bivarDens_s1_s1_r0.ps, width=0.39\linewidth, angle=270}
& &
\psfig{file=bivarContour_s1_s1_r0.ps, width=0.39\linewidth, angle=270}
\\
\psfig{file=bivarDens_s2_s1_r0.ps, width=0.39\linewidth, angle=270}
& &
\psfig{file=bivarContour_s2_s1_r0.ps, width=0.39\linewidth, angle=270}
\\
\psfig{file=bivarDens_s1_s1_r075.ps, width=0.39\linewidth, angle=270}
& &
\psfig{file=bivarContour_s1_s1_r075.ps, width=0.39\linewidth, angle=270}
\end{tabular}
\caption{Three instances of the bivariate normal distribution. Left panels: 3d plots of the bivariate density; right panels: corresponding contour plots. Top row: $\sigma_1^2=1$, $\sigma_2^2=1$, $\rho_{12}=0$; middle row: $\sigma_1^2=2$, $\sigma_2^2=1$, $\rho_{12}=0$; bottom row:
$\sigma_1^2=1$, $\sigma_2^2=1$, $\rho_{12}=0.75$.
\label{fig.bivariateNormal}}
\end{figure}

\end{example}



\subsection{The marginal and conditional distribution}
The \textit{marginal distribution} of a subset of the random variables $Y_1, \ldots, Y_p$ is the distribution of the random variables in the subset. More specifically, suppose the $p$ variates can be divided into two exhaustive and mutually exclusive subsets $\mathcal{A}$ and $\mathcal{B}$, i.e.: \textit{i)} $\mathcal{A}, \mathcal{B} \subset \{ 1, \ldots, p \}$, \textit{ii)} $\mathcal{A} \cap \mathcal{B} = \emptyset$, \textit{iii)} $\mathcal{A} \cup \mathcal{B} = \{ 1, \ldots, p\}$. Denote by $\mathbf{Y}_a$ and $\mathbf{Y}_b$ the random vectors that are obtained by restricting $\mathbf{Y}$ to the variates that correspond to the elements of subsets $\mathcal{A}$ and $\mathcal{B}$, respectively. The marginal density of $\mathbf{Y}_a$ is then:
\begin{eqnarray*}
f_{\mathbf{Y}_a}(\mathbf{y}_a) & = & \int_{\mathbb{R}^{| \mathcal{B} |}}
f_{\mathbf{Y}_a, \mathbf{Y}_b}(\mathbf{y}_a, \mathbf{y}_b) \, d \mathbf{y}_b,
\end{eqnarray*}
where $| \mathcal{B} |$ denotes the cardinality of the set $\mathcal{B}$.

\begin{ther}
If $\mathbf{Y} = (\mathbf{Y}_a^{\mathrm{T}}, \mathbf{Y}_b^{\mathrm{T}})^{\mathrm{T}}$ (with $\mathbf{Y}_a$ and $\mathbf{Y}_b$ as above) follows a multivariate normal distribution, i.e.:
\begin{eqnarray*}
\mathbf{Y} \, \, \, = \, \, \,
\left(
\begin{array}{c}
\mathbf{Y}_a
\\
\mathbf{Y}_b
\end{array}
\right)
& \sim & \mathcal{N} \left(
\left(
\begin{array}{c}
\mmu_a
\\
\mmu_b
\end{array}
\right),
\left(
\begin{array}{cc}
\SSigma_{aa} & \SSigma_{ab}
\\
\SSigma_{bb} & \SSigma_{bb}
\end{array}
\right)
\right),
\end{eqnarray*}
the marginal distribution of $\mathbf{Y}_a$ is $\mathcal{N}(\mmu_a, \SSigma_{aa}$).
\end{ther}

\begin{proof}
The proof is limited to $p=2$ (for the general case we refer to \reminder{REFERENCE}).
Let the 2-dimensional variable $(Y_1, Y_2)^{\mathrm{T}}$ follow a bivariate normal distribution. First normalize the variates individually: $\tilde{Y}_j  = (Y_j - \mu_j) / \sigma_j$ for $j=1, 2$. The joint density of $\tilde{Y}_1$ and $\tilde{Y}_2$ is then:
\begin{eqnarray*}
(2 \pi)^{-1} [(1-\rho^2)]^{-1/2}
\exp \left\{
- \frac{1}{2}
(1-\rho^2)^{-1}
\left(
\begin{array}{r}
\tilde{Y}_1
\\
\tilde{Y}_1
\end{array}
\right)^{\mathrm{T}}
\left(
\begin{array}{rr}
1 & -\rho
\\
-\rho & 1
\end{array}
\right)
\left(
\begin{array}{r}
\tilde{Y}_1
\\
\tilde{Y}_2
\end{array}
\right) \right\}.
\end{eqnarray*}
Next apply the following change-of-variables:
\begin{eqnarray*}
\left(
\begin{array}{c}
Z_1
\\
Z_2
\end{array}
\right)
& = &
\left(
\begin{array}{rr}
-C & -C
\\
-C & C
\end{array}
\right)
\left(
\begin{array}{c}
\tilde{Y}_1
\\
\tilde{Y}_2
\end{array}
\right) \qquad \mbox{ or } \qquad 
\left(
\begin{array}{c}
\tilde{Y}_1
\\
\tilde{Y}_2
\end{array}
\right)
\, \, \, = \, \, \, -\frac{1}{2} C^{-1}
\left(
\begin{array}{rr}
1 & 1
\\
1 & -1
\end{array}
\right)
\left(
\begin{array}{c}
Z_1
\\
Z_2
\end{array}
\right).
\end{eqnarray*}
The Jacobian, the absolute value of determinant of the matrix with all first order partial derivatives, of this transformation (the one on the right) equals: $J = (2C)^{-2}$ . The density function of $(Z_1, Z_2)$ is:
\begin{eqnarray*}
& & (2 \pi)^{-1} [(1-\rho^2)]^{-1/2}
\\
& & 
\exp \left\{ 
- \frac{1}{2}
[C /2 ]^{-2} (1-\rho^2)^{-1}
\left(
\begin{array}{r}
Z_1
\\
Z_2
\end{array}
\right)^{\mathrm{T}}
\left(
\begin{array}{rr}
1 & 1
\\
1 & -1
\end{array}
\right)
\left(
\begin{array}{rr}
1 & -\rho
\\
-\rho & 1
\end{array}
\right)
\left(
\begin{array}{rr}
1 & 1
\\
1 & -1
\end{array}
\right)
\left(
\begin{array}{r}
Z_1
\\
Z_2
\end{array}
\right)
\right\} 
\\
& = &  
(2 \pi)^{-1} [(1-\rho^2)]^{-1/2}
\\
& & \times 
\exp \left\{
-
[  C /2 ]^{-2} (1+\rho)^{-1} 
\left(
\begin{array}{r}
Z_1
\\
Z_2
\end{array}
\right)^{\mathrm{T}}
\left(
\begin{array}{rr}
1 & 0
\\
0 & 1 
\end{array}
\right)
\left(
\begin{array}{r}
Z_1
\\
Z_2
\end{array}
\right)
\right\}.
\end{eqnarray*}
The resulting bivariate random variable $ \mathbf{Z}$ is now standard normally distributed: $\mathcal{N}( \mathbf{0}_{2 \times 1}, \mathbf{I}_{2 \times 2})$. The independence of $Z_1$ and $Z_2$ enables the factorization of the integral in the calculation of the marginal distribution. BLAH BLAH.
\begin{eqnarray*}
f_{Y_1}(y_1) & = & \int_{-\infty}^{\infty} f_{(Y_1, Y_2)}(y_1, y_2) \, d y_2
\\
& = & (2 \pi)^{-1/2} \sigma_1^{-1} \exp[ - \frac{1}{2} (Y_1 - \mu_1)^2 / \sigma_1^2 ].
\end{eqnarray*}
The result is now evident.
\end{proof}

The importance of Theorem lies in the fact that the marginal normal distribution is itself (multivariate) normal. Consequently, we can interpret the parameters of the multivariate normal in terms of the marginal variances and (bivariate) correlations. For instance, if $\mathbf{Y}$ follows a $p$-variate normal distribution, then:
\begin{eqnarray*}
\mu_1 & = & E(Y_1) \, \, \, = \, \, \, \int_{-\infty}^{\infty} y_1 f_{Y_1}(y_1) \, dy_1 \, \, \, = \, \, \, \int_{\mathbb{R}^p} y_1 f_{\mathbf{Y}}(\mathbf{y}) \, d \mathbf{y}.
\end{eqnarray*}
Similar relations hold for the other parameters.



\begin{ther} \label{theo.cond.dist.normal}
Suppose a $p$-variate normally distributed random variabele $\mathbf{Y}$ can be partitioned as follows:
\begin{eqnarray*}
\mathbf{Y} \, \, \, = \, \, \,
\left(
\begin{array}{c}
\mathbf{Y}_a
\\
\mathbf{Y}_b
\end{array}
\right)
& \sim & \mathcal{N} \left(
\left(
\begin{array}{c}
\mmu_a
\\
\mmu_b
\end{array}
\right),
\left(
\begin{array}{cc}
\SSigma_{aa} & \SSigma_{ab}
\\
\SSigma_{ba} & \SSigma_{bb}
\end{array}
\right)
\right).
\end{eqnarray*}
The conditional distribution of $\mathbf{Y}_a | \mathbf{Y}_b$ is then:
\begin{eqnarray*}
\mathbf{Y} | \mathbf{X} & = &  \mathcal{N}(\mmu_Y + \SSigma_{YZ} \SSigma_{XX}^{-1} (\mathbf{X} - \mmu_X),
\SSigma_{YY} - \SSigma_{YX} \SSigma_{XX}^{-1} \SSigma_{XY}).
\end{eqnarray*}
\end{ther}

\begin{proof}
Refer Theorem B.6.5 of \cite{Bick2001} or \reminder{ANDERSON}
\end{proof}




\begin{example}
Consider the trivariate normal distribution:
\begin{eqnarray*}
\left(
\begin{array}{l}
Y_{1}
\\
Y_{2}
\\
Y_{3}
\end{array}
\right)
& \sim &
\mathcal{N} \left(
\left(
\begin{array}{l}
0
\\
0
\\
0
\end{array}
\right),
\left(
\begin{array}{lll}
2 & -1 & -1
\\
-1 & 3/2 & 1/2
\\
-1 & 1/2 & 3/2
\end{array}
\right)
\right).
\end{eqnarray*}
The marginal distribution of $(Y_2, Y_3)^{\mathrm{T}}$
\begin{eqnarray*}
\left(
\begin{array}{l}
Y_{2}
\\
Y_{3}
\end{array}
\right)
& \sim &
\mathcal{N} \left(
\left(
\begin{array}{l}
0
\\
0
\end{array}
\right),
\left(
\begin{array}{ll}
3/2 & 1/2
\\
1/2 & 3/2
\end{array}
\right)
\right).
\end{eqnarray*}
To obtain the conditional distribution of $(Y_2, Y_3)^{\mathrm{T}}$ on $Y_1$ apply Theorem \ref{theo.cond.dist.normal}. For the conditional mean, we get:
\begin{eqnarray*}
\mmu_{ (Y_2, Y_3) \, | \, Y_1} & = & \left(
\begin{array}{l}
0
\\
0
\end{array}
\right) +
\left(
\begin{array}{l}
-1
\\
-1
\end{array}
\right) \, \left( \frac{1}{2} \right)
\, ( Y_1 - 0).
\end{eqnarray*}
The conditional variance equals:
\begin{eqnarray*}
\SSigma_{ (Y_2, Y_3) \, | \, Y_1} & = & \left(
\begin{array}{ll}
3/2 & 1/2
\\
1/2 & 3/2
\end{array}
\right) -
\left(
\begin{array}{l}
-1
\\
-1
\end{array}
\right)
\left( \frac{1}{2} \right)
\left(
\begin{array}{r}
-1
\\
-1
\end{array}
\right)^{\mathrm{T}}.
\end{eqnarray*}
The conditional distribution of $(Y_2, Y_3)^{\mathrm{T}}$ on $Y_1$ is thus:
\begin{eqnarray*}
\left.
\left(
\begin{array}{l}
Y_{1}
\\
Y_{2}
\end{array}
\right) \,  \right| \, Y_1
& \sim &
\mathcal{N} \left(
\left(
\begin{array}{l}
- \frac{1}{2 } Y_1
\\
- \frac{1}{2 } Y_1
\end{array}
\right),
\left(
\begin{array}{ll}
1 & 0
\\
0 & 1
\end{array}
\right)
\right).
\end{eqnarray*}
Hence, conditional on $Y_1$, the random variable $Y_2$ and $Y_3$ are uncorrelated. \reminder{Add picture.}
\\
\\
Let the random variables $Y_1$, $Y_2$ and $Y_3$ represent expression levels of three genes. As $Y_2$ and $Y_3$ are independent given $Y_1$, their conditional independence graph is given by edge set $\mathcal{E} = \{ Y_2 - Y_1, Y_1 - Y_3 \}$. Would high expression levels of $Y_2$ be essential for the viability of the cell and $Y_3$ be a nuisance variable, one may neutralize the effect of $Y_3$ by controlling $Y_1$.
\end{example}


\subsection{Estimation of $\mmu$ and $\SSigma$}
Consider an experiment involving a sample of $n$ individuals of which a $p$ traits are measured, denoted $\mathbf{y}_{\ast,i}$ for sample $ = 1, \ldots, n$. Assuming the $\mathbf{y}_{\ast,i}$ are realizations from a multivariate normal distribution, the data may be used to estimate the parameters $\mmu$ and $\SSigma$ of the distribution. To derive the maximum likelihood estimate, we consider the log-likelihood:
\begin{eqnarray*}
\mathcal{L}(\mathbf{Y}_{\ast, 1}, \ldots, \mathbf{Y}_{\ast, n}) & = & \sum_{i=1}^n \log [ f_{\mathbf{Y}}(\mathbf{y}_{\ast,i})]
\\
& \propto &  - \frac{n}{2} \log ( | \SSigma | ) - \frac{1}{2} \sum_{i=1}^n (\mathbf{y}_{\ast,i} - \mmu)^{\mathrm{T}} \SSigma^{-1} (\mathbf{y}_{\ast,i} - \mmu)
\\
& = &  - \frac{n}{2} \log ( | \SSigma | ) - \frac{1}{2} \sum_{i=1}^n \mbox{tr}[ (\mathbf{y}_{\ast,i} - \mmu)^{\mathrm{T}} \SSigma^{-1} (\mathbf{y}_{\ast,i} - \mmu)]
\\
& = &  - \frac{n}{2} \log ( | \SSigma | ) - \frac{1}{2} \sum_{i=1}^n \mbox{tr}[ (\mathbf{y}_{\ast,i} - \mmu) (\mathbf{y}_{\ast,i} - \mmu)^{\mathrm{T}} \SSigma^{-1} ]
\\
& = &  - \frac{n}{2} \log ( | \SSigma | ) - \frac{1}{2}  \mbox{tr} \Big[ \sum_{i=1}^n (\mathbf{y}_{\ast,i} - \mmu) (\mathbf{y}_{\ast,i} - \mmu)^{\mathrm{T}} \SSigma^{-1} \Big]
\\
& = &  - \frac{n}{2} \log ( | \SSigma | ) - \frac{n}{2}  \mbox{tr}( \mathbf{S} \SSigma^{-1} ),
\end{eqnarray*}
where
\begin{eqnarray*}
\mathbf{S} & = & \frac{1}{n} \sum_{i=1}^n (\mathbf{y}_{\ast,i} - \mmu) (\mathbf{y}_{\ast,i} - \mmu)^{\mathrm{T}},
\end{eqnarray*}
the sample covariance matrix. In the derivation of the log-likelihood we used {\it i)} $a  = \mbox{tr}(a)$ for $a$ a scalar, {\it ii)} the cyclic property of the trace: $\mbox{tr}( \mathbf{A} \mathbf{B} )   = \mbox{tr}( \mathbf{B} \mathbf{A} )$, and {\it iii)} the additive property of the trace: $\mbox{tr}( \mathbf{A}) + \mbox{tr}( \mathbf{B} ) = \mbox{tr}( \mathbf{A} \mathbf{B} )$.

For the derivation of the ML estimate of $\mmu$ take the derivative with respect to $\mmu$:
\begin{eqnarray*}
\frac{\partial \,}{\partial \, \mmu}  \mathcal{L}(\mathbf{Y}_{\ast, 1}, \ldots, \mathbf{Y}_{\ast, n} )
& \propto &   \sum_{i=1}^n \frac{\partial \,}{\partial \, \mmu}  [(\mathbf{y}_{\ast,i} - \mmu)^{\mathrm{T}} \SSigma^{-1} (\mathbf{y}_{\ast,i} - \mmu)]
\\
& = &  - \sum_{i=1}^n [ \SSigma^{-1} (\mathbf{y}_{\ast,i} - \mmu)]^{\mathrm{T}}
+ (\mathbf{y}_{\ast,i} - \mmu)^{\mathrm{T}} \SSigma^{-1}
\\
& = &  - 2 \Big[ \sum_{i=1}^n (\mathbf{y}_{\ast,i} - \mmu)^{\mathrm{T}}
\Big] \SSigma^{-1},
\end{eqnarray*}
where we have used the chain rule and, for $\mathbf{A}$ independent of $\mathbf{y}$, that
\begin{eqnarray*}
\frac{\partial \,  \mathbf{A} \mathbf{y}}{ \partial \, \mathbf{y}} & = &  \mathbf{A}
\qquad \mbox{ and } \qquad
\frac{\partial \, \mathbf{y}^{\mathrm{T}} \mathbf{A}}{ \partial \, \mathbf{y}} \, \,\, = \, \, \,  \mathbf{A}^{\mathrm{T}}.
\end{eqnarray*}
Equate the derivative to zero and solve for $\mmu$. This yields:
\begin{eqnarray*}
\hat{\mmu}_{ML} & = & \frac{1}{n} \sum_{i=1}^n \mathbf{Y}_{\ast, i}.
\end{eqnarray*}
This coincides with the univariate estimates of the mean.

The ML estimate of $\SSigma$ is:
\begin{eqnarray*}
\mbox{arg} \max_{\SSigma > 0} \frac{n}{2} \log ( | \SSigma^{-1} | ) - \frac{n}{2}  \mbox{tr}( \mathbf{S} \SSigma^{-1} ).
\end{eqnarray*}
To solve this apply the change-of variable $\OOmega = \SSigma^{-1}$, which gives:
\begin{eqnarray*}
\mbox{arg} \max_{\OOmega > 0} \frac{n}{2} \log ( | \OOmega | ) - \frac{n}{2}  \mbox{tr}( \mathbf{S} \OOmega ).
\end{eqnarray*}
Equate the derivative with respect to $\OOmega$ to zero:
\begin{eqnarray*}
\frac{n}{2} \OOmega^{-1} - \frac{n}{2}  \mathbf{S}  & = & \mathbf{0}_{p \times p},
\end{eqnarray*}
in which we have used that 
\begin{eqnarray*}
\frac{\partial }{\partial \mathbf{X}} \log ( | \mathbf{X} | ) & = & \mathbf{X}^{-1} \qquad \mbox{ and } \qquad 
\frac{\partial }{\partial \mathbf{X}} \mbox{tr} ( \mathbf{A} \mathbf{X}  ) \, \, \, = \, \, \, \mathbf{A}.
\end{eqnarray*}
Solving this equation for $\OOmega$ yields the maximum likelihood estimate $\hat{\SSigma}_{ML} =  \mathbf{S}$.


\begin{example}
Simple data set?
\end{example}

\subsection{Convolution of multivariate normals}


\subsection{Estimation of $\SSigma$ when $p > n$}



\newpage
\section{Partial correlation}
\noindent
The partial correlation coefficient quantifies the correlation between two variables when conditioning on one or several other variables. 
More formally, let $\mathbf{Y} = (Y_1, \ldots, Y_p)^{\mathrm{T}}$ be a $p$-variate random variable, 
$a, b$ elements of $\{1, \ldots, p\}$, $\mathcal{C}$ a subset of $\{1, \ldots, p\}$,
$Y_a$ and $Y_b$ the corresponding variates of $\mathbf{Y}$, and $\mathbf{Y}_c$ obtained by limiting $\mathbf{Y}$ to the variates with an index in $\mathcal{C}$.
The partial correlation coefficient between $Y_a$ and $Y_b$ conditional on $\mathbf{Y}_{c}$ is the correlation between the residuals of $Y_a$ and $Y_b$ after regressing them on $\mathbf{Y}_c$. 
Or, 
% The partial correlation coefficient between $Y_a$ and $Y_b$ conditional on variables $\mathbf{Y}_c$ is defined as:
\begin{eqnarray}  \nonumber
\rho (Y_a, Y_b \, | \, \mathbf{Y}_c ) & = & \frac{\mbox{Cov}(Y_a, Y_b \, | \, \mathbf{Y}_c)}
{\sqrt{\mbox{Var}(Y_a \, | \, \mathbf{Y}_c)} \, \sqrt{\mbox{Var}(Y_b \, | \, \mathbf{Y}_c) }}
\\
\label{form.partCorDef}
& = & \frac{E(Y_a, Y_b \, | \, \mathbf{Y}_c) - E(Y_a \, | \, \mathbf{Y}_c) \, E(Y_b \, | \, \mathbf{Y}_c)}
{\sqrt{E(Y_a^2 \, | \, \mathbf{Y}_c) - E(Y_a \, | \, \mathbf{Y}_c) \, E(Y_a \, | \, \mathbf{Y}_c)} \,
 \sqrt{E(Y_b^2 \, | \, \mathbf{Y}_c) - E(Y_b \, | \, \mathbf{Y}_c) \, E(Y_b \, | \, \mathbf{Y}_c)}}.
\end{eqnarray}
The order of the partial correlation coefficient is determined by the number of variables (that is, the cardinality of $\mathcal{C}$) it is conditional on.

The partial correlation coefficient may be defined recursively. Let the zeroth-order partial correlation coefficient be defined as:
\begin{eqnarray*}
\rho ( Y_a, Y_b \, | \, \emptyset ) & = & \frac{\mbox{Cov}(Y_a, Y_b)}{\sqrt{\mbox{Var}(Y_a)} \, \sqrt{\mbox{Var}(Y_b)}}.
\end{eqnarray*}
The higher order partial correlation coefficients are then defined recursively as:
\begin{eqnarray*}
\rho ( Y_a, Y_b \, | \, \mathbf{Y}_c ) & = & \frac{ \rho (Y_a, Y_b \, | \, \mathbf{Y}_{c \setminus d  } )
- \rho ( Y_a, Y_d \, | \, \mathbf{Y}_{c \setminus d } )   \, \rho (Y_d, Y_b \, | \, \mathbf{Y}_{c \setminus d })
}{ \sqrt{ 1 - \rho ( Y_a, Y_d \, | \, \mathbf{Y}_{c \setminus d } ) } \, \sqrt{ 1 - \rho ( Y_d, Y_b \, | \, \mathbf{Y}_{c \setminus d } ) } } ,
\end{eqnarray*}
where $d$ is a element in $\mathcal{C}$.

A natural estimator of the partial correlation is obtained by replacing of the conditional expectations in definition (\ref{form.partCorDef}) by the corresponding estimates:
\begin{eqnarray*}
\hat{\rho} (Y_a, Y_b \, | \, \mathbf{Y}_c ) & = & \frac{\sum e_{Y_a \, | \, \mathbf{Y}_c} \, e_{Y_b \, | \, \mathbf{Y}_c} - \big( \sum e_{Y_a \, | \, \mathbf{Y}_c} \big)
\big( \sum e_{Y_b \, | \, \mathbf{Y}_c} \big)}{
\sqrt{ \sum e_{Y_a \, | \, \mathbf{Y}_c}^2 - \big( \sum e_{Y_a \, | \, \mathbf{Y}_c} \big)^2} \,
\sqrt{ \sum e_{Y_b \, | \, \mathbf{Y}_c}^2 - \big( \sum e_{Y_b \, | \, \mathbf{Y}_c} \big)^2}
},
\end{eqnarray*}
in which, e.g. $e_{Y_a \, | \, \mathbf{Y}_c} = Y_a - E(Y_a \, | \, \mathbf{Y}_c)$, the residual obtained when regressing $Y_a$ on $\mathbf{Y}_c$.
\\
\\
\\
\\
An alternative method to calculate the partial correlation coefficient is by inversion of the correlation matrix of $\SSigma$:
\begin{eqnarray*}
\rho (Y_a,  Y_b \, | \, \mathbf{Y}_c) & = & \frac{\big( \SSigma^{-1} \big)_{a, b}}{\sqrt{ \big( \SSigma^{-1} \big)_{Y_a, Y_b} } \, \sqrt{ \big( \SSigma^{-1} \big)_{Y_a, Y_b} }}.
\end{eqnarray*}

\begin{lemma} (Inverse Variance Lemma)
\\
Let $\mathbf{Y}_a$ and $\mathbf{Y}_b$ be $p_1$- and $p_2$-dimensional random variables. The inverse of the partitioned variance $\mbox{Var}[ (\mathbf{Y}_a^{\mathrm{T}}, \mathbf{Y}_b^{\mathrm{T}})^{\mathrm{T}}]$ is:
\begin{eqnarray*}
\left\{ \mbox{Var} \left[ \left( \begin{array}{l} \mathbf{Y}_a \\ \mathbf{Y}_b \end{array} \right) \right] \right\}^{-1} & = &  
\left( 
\begin{array}{rr} 
[\mbox{Var}(\mathbf{Y}_a )]^{-1} + \mathbf{B}^{\mathrm{T}} [\mbox{Var}(\mathbf{Y}_b  \, | \, \mathbf{Y}_a )]^{-1} \mathbf{B}
&
- \mathbf{B}^{\mathrm{T}} [\mbox{Var}(\mathbf{Y}_b  \, | \, \mathbf{Y}_a )]^{-1}
\\ 
- [\mbox{Var}(\mathbf{Y}_b  \, | \, \mathbf{Y}_a )]^{-1} \mathbf{B} & [\mbox{Var}(\mathbf{Y}_b  \, | \, \mathbf{Y}_a )]^{-1}
\end{array} 
\right),
\end{eqnarray*}
where $\mathbf{B} = \mbox{Cov}( \mathbf{Y}_a, \mathbf{Y}_b ) [\mbox{Var}(\mathbf{Y}_a )]^{-1}$.
\end{lemma}
\begin{proof}
We now show how this equality follows.
Write $\mathbf{X} = (X_1, X_2)^T$ and assume
\begin{eqnarray*}
\left( \begin{array}{c} \mathbf{Z} \\ \mathbf{X} \end{array} \right) & \sim &
N \left( \left( \begin{array}{c} \mmu_Z \\ \mmu_X \end{array} \right),
\left( \begin{array}{cc} \SSigma_{ZZ} & \SSigma_{XZ} \\ \SSigma_{ZX} & \SSigma_{XX} \end{array} \right) \right).
\end{eqnarray*}
Theorem B.6.5 in Bickel, Doksum (2000) then states:
\begin{eqnarray*}
\mathbf{X} \, | \, \mathbf{Z} & \sim &
N (\mmu_X + \SSigma_{XZ} \, \SSigma_{ZZ}^{-1} \, (\mathbf{Z} - \mmu_Z), \SSigma_{XX} - \SSigma_{XZ} \, \SSigma_{ZZ}^{-1} \, \SSigma_{ZX}).
\end{eqnarray*}
In addition, from exercise 2.7 in Rao (1973) we have:
\begin{eqnarray*}
\left( \begin{array}{cc} \mathbf{A} & \mathbf{B} \\ \mathbf{B}^T & \mathbf{D} \end{array} \right)^{-1}
& = &
\left( \begin{array}{cc} \mathbf{A}^{-1} + \mathbf{F} \, \mathbf{E}^{-1} \, \mathbf{F}^T  & - \mathbf{F} \mathbf{E}^{-1}
\\
- \mathbf{E}^{-1} \, \mathbf{F}^T &  \mathbf{E}^{-1},
\end{array}
\right)
\end{eqnarray*}
where $\mathbf{E} = \mathbf{D} - \mathbf{B}^T \, \mathbf{A}^{-1} \, \mathbf{B}$ and
$\mathbf{F} = \mathbf{A}^{-1} \, \mathbf{B}$.
This shows that $\mbox{Var}(\mathbf{X} \, | \, \mathbf{Z})$ equals the relevant submatrix of the inverse of $\SSigma$.
\end{proof}


\begin{coro} (Corollary 5.8.1, \cite{Whit1991} )
Each diagonal element of the inverse covariance matrix is the reciprocal of a partial variance. 
\end{coro}
\begin{proof}
In the Inverse Variance Lemma, set $p_2 = 1$. Then:
\begin{eqnarray*}
[\mbox{Var}(Y_b \, | \, \mathbf{Y}_a)]^{-1} & = & 1 / \mbox{Var}(Y_b \, | \, \mathbf{Y}_a)
\end{eqnarray*}
for this is a scalar. This holds for any diagonal element as any can be selected by permutation of the original random vector.
\end{proof}



\begin{coro} (Corollary 5.8.2, \cite{Whit1991} )
Each off-diagonal element of the inverse variance matrix (scaled to have a unit diagonal) is the negative of the partial correlation between the two corresponding variables, conditioned on all remaining variables.
\end{coro}
\begin{proof}
In the Inverse Variance Lemma, set $p_2 = 2$. In the remainder of the proof let $\mathbf{Y}_b = (Y_{b_1}, Y_{b_2})^{\mathrm{T}}$. Then:
\begin{eqnarray*}
[\mbox{Var}(\mathbf{Y}_b \, | \, \mathbf{Y}_a)]^{-1} & = & \mathbf{D}
\, \, \, := \, \, \,
\left( 
\begin{array}{rr}
d_{11} & d_{12}
\\
d_{21} & d_{22}
\end{array}
\right).
\end{eqnarray*}
Turning this around yields:
\begin{eqnarray*}
\left(
\begin{array}{rr}
\mbox{Var}(Y_{b_1} \, | \, \mathbf{Y}_a) & \mbox{Cov}(Y_{b_1}, Y_{b_2} \, | \, \mathbf{Y}_a)
\\
\mbox{Cov}(Y_{b_1}, Y_{b_2} \, | \, \mathbf{Y}_a) & \mbox{Var}(Y_{b_2} \, | \, \mathbf{Y}_a)
\end{array}
\right)
& = & 
\left(
\begin{array}{rr}
d_{11} & d_{12}
\\
d_{21} & d_{22}
\end{array}
\right)^{-1}
\\
& = & 
\frac{1}{d_{11} d_{22} - d_{12} d_{21}}
\left(
\begin{array}{rr}
d_{22} & -d_{21}
\\
-d_{12} & d_{11}
\end{array}
\right).
\end{eqnarray*}
Rescale this to have a unit diagonal and one obtains:
\begin{eqnarray*}
\frac{-d_{12}}{\sqrt{d_{11} d_{22}}} & = & \frac{\mbox{Cov}(Y_{b_1}, Y_{b_2} \, | \, \mathbf{Y}_a)}
{\sqrt{ \mbox{Var}(Y_{b_1} \, | \, \mathbf{Y}_a)} \, \sqrt{\mbox{Var}(Y_{b_2} \, | \, \mathbf{Y}_a)}},
\end{eqnarray*}
which matches the definition of partial correlation. Permutation extends the result to all off-diagonal elements.
\end{proof}


\begin{example} \label{exam.trivariateNormal}
\textit{Illustration of the calculation of the partial correlation via inversion of the covariance matrix.}
Consider the $3 \times 3$ covariance matrix (without loss of generality rescaled to be a correlation matrix):
\begin{eqnarray*}
\SSigma & = & \left(
\begin{array}{ccc}
1 & \rho_{12} & \rho_{13} \\
\rho_{12} & 1 & \rho_{23} \\
\rho_{13} & \rho_{23} & 1
\end{array}
\right).
\end{eqnarray*}
The corresponding precision matrix is then:
\begin{eqnarray*}
\SSigma^{-1} & = & \frac{1}{\mbox{det}(\SSigma)} \left(
\begin{array}{ccc}
1 - \rho_{23}^2 & \rho_{13} \, \rho_{23} - \rho_{12} & \rho_{12} \, \rho_{23} - \rho_{13}  \\
\rho_{13} \, \rho_{23} - \rho_{12} & 1 - \rho_{13}^2 & \rho_{12} \, \rho_{13} - \rho_{23}  \\
\rho_{12} \, \rho_{23} - \rho_{13} & \rho_{12} \, \rho_{13} - \rho_{23} & 1 - \rho_{12}^2
\end{array}
\right),
\end{eqnarray*}
which is (up a factor of the determinant of $\SSigma$) identical to:
\begin{eqnarray*}
\mbox{Var}(\mathbf{X} \, | \, Z) \, \, \, = \, \, \, \SSigma_{XX} - \SSigma_{XZ} \, \SSigma_{ZZ}^{-1} \, \SSigma_{ZX} & = & \left(
\begin{array}{cc}
1 & \rho_{12} \\
\rho_{12} & 1\\
\end{array}
\right) - \left(
\begin{array}{cc}
\rho_{13}^2 & \rho_{13} \, \rho_{23} \\
\rho_{13} \, \rho_{23} & \rho_{23}^2.
\end{array}
\right)
\end{eqnarray*}
This factor cancels out when calculating the partial correlation coefficient:
\begin{eqnarray*}
\rho_{12.3} & = & \frac{\big( \SSigma^{-1} \big)_{12}}{\sqrt{ \big( \SSigma^{-1} \big)_{11} } \, \sqrt{ \big( \SSigma^{-1} \big)_{22} }}
\, \, \, = \, \, \,  \frac{\rho_{13} \, \rho_{23} - \rho_{12}}{\sqrt{1 - \rho_{23}^2} \, \sqrt{1 - \rho_{13}^2}}.
\end{eqnarray*}
\end{example}



\begin{coro} \label{cor.parametricIndependenceCriterion}
Let $\mathbf{Y}_{a}$, $\mathbf{Y}_{b}$ and $\mathbf{Y}_{c}$ by $p_1$-, $p_2$- and $p_3$-dimensional normally distributed random vectors. Then,  $\mathbf{Y}_{a}$ and $\mathbf{Y}_{b}$ 
are independent conditional on $\mathbf{Y}_{c}$, i.e.  $\mathbf{Y}_{a} \perp \mathbf{Y}_{b} \, | \,  \mathbf{Y}_{c}$, if and only if:
\begin{eqnarray*}  
(\OOmega)_{a,b} & = & \mathbf{0}_{|\mathcal{A}| \times |\mathcal{B}|},
\end{eqnarray*}
where $\OOmega = \{ \mbox{Var}[ (\mathbf{Y}_a, \mathbf{Y}_b, \mathbf{Y}_c)] \}^{-1}$.
\end{coro}
This gives a simple parametric criterion for (conditional) pairwise independence.




\begin{example}
\textit{Illustration of equivalence of a zero partial correlation and conditional independence.}
Consider the $3 \times 3$ covariance matrix (without loss of generality rescaled to be a correlation matrix) of a trivariate normal distribution (as in Example \ref{exam.trivariateNormal}).
Its inverse $\OOmega$, the precision matrix, is then as in Example \ref{exam.trivariateNormal}. Now assume $(\OOmega)_{13} = \rho_{12} \rho_{23} - \rho_{13} = 0$. Corollary \ref{cor.parametricIndependenceCriterion} then says $Y_1$ and $Y_3$ are independent conditional on $Y_2$: $Y_1 \perp Y_3 \, | \, Y_2$. This suggests that the joint density of $Y_1$, $Y_2$ and $Y_3$ would factorize as implied by Proposition \reminder{WELKE DAN?}. This is indeed the case, confer:
\begin{eqnarray*}
& & \hspace{-1.5cm} f_{(Y_1, Y_2, Y_3)}(y_1, y_2, y_3) 
\\
& = & C \, \exp( - \frac{1}{2} \mathbf{y}^{\mathrm{T}} \OOmega \mathbf{y} )
\\
& = & C \, \exp \{ - \frac{1}{2} [ (\OOmega)_{11} \, y_1^2  + (\OOmega)_{22} \, y_2^2 + (\OOmega)_{33} \, y_3^2 + 2 \, (\OOmega)_{12} \, y_1 y_2 + 2 \, (\OOmega)_{23} \, y_2 y_3]  \}
\\
& = & C \, \exp \{ - \frac{1}{2} [ (\OOmega)_{11} \, y_1^2  + 2 \, (\OOmega)_{12} \, y_1 y_2 + (\OOmega)_{22} \, y_2^2 ] \} 
\\
& & \times \, \exp \{ - \frac{1}{2} [  2 \, (\OOmega)_{33} \, y_3^2 + (\OOmega)_{23} \, y_2 y_3]  \}
\\
& = & g(y_1, y_2) \, h(y_3, y_2),
\end{eqnarray*}
for some suitably chosen functions $g(\cdot, \cdot)$ and $h(\cdot, \cdot)$
\end{example}


\subsection{Relation to regression}





\newpage
\section{A single molecular level}
\subsection{Experimental design, data, and notation}
We consider integrative oncogenomics studies with a time-course set-up that aim to elucidate the molecular mechanisms within the cancer cell. To this end an integrative oncogenomics study profiles {\it multiple} molecular levels of the same sample in high-throughput fashion using microarrays. The time-course set-up implies that each sample included in the study is followed over time and, at multiple time points during this period, interrogated molecularly.

% Time-course studies may be more powerful to investigate (dynamical aspects of) the regulatory system, for ethical reasons they are conducted only {\it in vitro}. In the particular, but representative, time-course, integrative oncogenomic experiment an human papilloma virus (HPV), a carcinogenic entity, is introduced in a healthy cell (from a specific lineage). After introduction of the virus the cell is placed in a petri dish with enough food and allowed to grow. Once the cell and its offspring have filled the petri dish, a sample from filled petri dish is placed in an empty one. Such a transplantation is called a passage. This process continues for many passages (here over 200). From this description it should be clear that we

In the experiment we assume that $n$ samples are followed over time. The expression levels of $p$ genes of sample $i$ are measured at $\mathcal{T}$ time points. The time points are assumed to be identical for each sample. Let $Y_{j,i,t}$ be the zero-centered expression level of gene $j$ in sample $i$ at time point $t$. The matrix of all gene expression profiles of all samples at time point $t$ is denoted:
\begin{eqnarray*}
\mathbf{Y}_{\ast, \ast, t} & = & \big(\mathbf{Y}_{\ast, 1, t} \, | \,  \mathbf{Y}_{\ast,2,t} \, | \,  \ldots \, | \,  \mathbf{Y}_{\ast, n, t} \big) \, \, \, = \, \, \,
\left(
\begin{array}{cccc}
Y_{1,1,t} & Y_{1,2,t} & \ldots & Y_{1,n,t}
\\
Y_{2,1,t} & Y_{2,2,t} & \ldots & Y_{2,n,t}
\\
\vdots & \vdots & \ddots & \vdots
\\
Y_{p,1,t} & Y_{p,2,t} & \ldots & Y_{p,n,t}
\end{array}\right)
\end{eqnarray*}
Similarly, the matrix of all gene expression profiles of sample $i$ is written as:
\begin{eqnarray*}
\mathbf{Y}_{\ast, i, \ast} & = &
\big(\mathbf{Y}_{\ast, i, 1} \, | \,  \mathbf{Y}_{\ast, i, 2} \, | \,  \ldots \, | \, \mathbf{Y}_{\ast, i, \mathcal{T}} \big) \, \, \, = \, \, \,
\left(
\begin{array}{cccc}
Y_{1,i,1} & Y_{1,i,2} & \ldots & Y_{1,i,\mathcal{T}}
\\
Y_{2,i,1} & Y_{2,i,2} & \ldots & Y_{2,i,\mathcal{T}}
\\
\vdots & \vdots & \ddots & \vdots
\\
Y_{p,i,1} & Y_{p,i,2} & \ldots & Y_{p,i,\mathcal{T}}
\end{array}\right)
\end{eqnarray*}
% Further:
% \begin{eqnarray*}
% \mbox{vec}(\mathbf{Y}_{\ast i}^{(\ast)}) & = & (
% Y_{1i}^{(1)}, \ldots, Y_{pi}^{(1)}, Y_{1i}^{(2)}, \ldots, Y_{pi}^{(2)}, \ldots, Y_{1i}^{(T)}, \ldots, Y_{pi}^{(T)})^T
% \end{eqnarray*}
At each time point each sample is also interrogated genomically. This yields a collection of DNA copy number profiles. An element of these profiles is denoted  $X_{jit}$, the DNA copy number corresponding to gene $j$ in sample $i$ and time point $t$. In analogous fashion as above we write the matrix of the samples' DNA copy number data at a particular time point, and the matrix of genomic profiles of a particular sample over time.
\\
\\
Alternatively, $X_{j,i,t}$ may represent microRNA expression. Notice that the matrix $\mathbf{X}_{\ast, \ast, t}$ may then be of different dimensions, that is, may have a different number of rows.
\\
\\
The typical sample size of time-course experiments in terms of the number of time points sampled hardly ever exceeds ten. Usually, the number of cell lines involved in the experiments is also very small, say, a handful maximum. \reminder{(ref for these numbers.)}






\subsection{The VAR(1) model}
The gene expression data from the time-course experiment is modeled by a VAR(1) (first-order vector autoregressive) process:
\begin{eqnarray} \label{form.var1}
\mathbf{Y}_{\ast, i, t} \, | \, \mathbf{Y}_{\ast, i,t-1}, \ldots,  \mathbf{Y}_{\ast, i, 1} & = & \nnu + \mathbf{A} \mathbf{Y}_{\ast, i, t-1} + \vvarepsilon_{\ast, i, t},
\end{eqnarray}
where $\nnu$ the $p \times 1$ intercept vector, $\mathbf{A}$ a $p \times p$ coefficient matrix, and $\vvarepsilon_{\ast,i,t}$ a $p \times 1$ vector with the errors. Throughout it is assumed that $\vvarepsilon_{\ast,i,t} \sim \mathcal{N}(\mathbf{0}_{p \times 1}, \mathbf{\Sigma}_{\varepsilon})$ and $\mbox{Cov}(\vvarepsilon_{\ast,i_1,t_1}, \vvarepsilon_{\ast,i_2,t_2}) = \mathbf{0}$ if $t_1 \not=t_2$ or $i_1 \not=i_2$. Thus, for a 3-gene pathway model (\ref{form.var1}) becomes:
\begin{eqnarray*}
\left\{
\begin{array}{ccc}
Y_{1,i,t} & = & \nu_1 + a_{11} Y_{1,i,t-1} + a_{12} Y_{2,i,t-1} + a_{13} Y_{3,i,t-1} + \varepsilon_{1,i,t},
\\
Y_{2,i,t} & = & \nu_2 + a_{21} Y_{1,i,t-1} + a_{22} Y_{2,i,t-1} + a_{23} Y_{3,i,t-1} + \varepsilon_{2,i,t},
\\
Y_{3,i,t} & = & \nu_3 + a_{31} Y_{1,i,t-1} + a_{32} Y_{2,i,t-1} + a_{33} Y_{3,i,t-1} + \varepsilon_{3,i,t},
\end{array}
\right.
\end{eqnarray*}
where for notational simplicity we have dropped the conditioning on the past at the left hand-side of the equations.
\\
\\
VAR(1) models have already been applied to modeling gene expression data from a time-course experiment in \cite{Fuji2007}. This comprises a straightforward application of a VAR(1) model with a lasso penalty.
\\
\\
Also \cite{Abeg2013} applied it. Summarize briefly.


\begin{example} \label{example.feedforward.motif}
\textit{Illustration of the VAR(1) model.}
Consider the feed-forward motif, apparently a common motif in the transcriptional network \citep{Alon2007}. The feed-forward motif comprises three genes. Let the random variables $Y_1$, $Y_2$ and $Y_3$ represent their gene expression levels. The feed-forward motif is then defined by the edge set $\mathcal{V}  = \{ Y_1 \rightarrow Y_2, Y_1 \rightarrow Y_3, Y_2 \rightarrow Y_3 \}$. Over both paths from $Y_1$ to $Y_3$ the overall sign should be the same. The motif is depicted in Figure \ref{fig.feedforward.motif}.


The particular version of the feed-forward motif used to generate the data depicted in Figure \ref{fig.feedforward.data} is given by the following model:
\begin{eqnarray} \label{form.var1.3gene.feedforward}
\left\{
\begin{array}{rcrcrcr}
Y_{1,i,t} & = & & & & & \varepsilon_{1,i,t},
\\
Y_{2,i,t} & = & -\frac{5}{2} Y_{1,i,t-1} &  & & + & \varepsilon_{2,i,t},
\\
Y_{3,i,t} & = & \frac{9}{5} Y_{1,i,t-1} & - & \frac{3}{2} Y_{2,i,t-1} & + & \varepsilon_{3,i,t}.
\end{array}
\right.
\end{eqnarray}
in which $\SSigma_{\varepsilon} = \frac{1}{4} \mathbf{I}_{p \times p}$.


\begin{figure}[h!]
\begin{center}
\includegraphics[angle=0, scale=0.50]{feedforwardMotif.eps}
\end{center}
\caption{On the left-hand side: schemata of the feed-forward motif. To the right, the feed-forward motif unfolded. \label{fig.feedforward.motif}}
\end{figure}

\begin{figure}[h!]
\begin{center}
\includegraphics[angle=270, scale=0.50]{feedforward3gene_data.ps}
\end{center}
\caption{Data from  the feed-forward example as specified by model (\ref{form.var1.3gene.feedforward}). The dashed lines connect the data points, the solid line is a smoothing of the data to emphasize the trend. \label{fig.feedforward.data}}
\end{figure}
\clearpage

\end{example}


\subsubsection{Stability and stationarity}
The VAR(1) process is said to be {\it stable} if all eigenvalues of $\mathbf{A}$ have modulus less than one, i.e. $|\lambda_j(\mathbf{A})| < 1$ for $j=1,\ldots,p$. This is equivalent to:
\begin{eqnarray*}
\mbox{det}( \mathbf{I}_{p \times p} - \mathbf{A} \, \mathbf{x}) \not= 0  \qquad \mbox{for } | \mathbf{x} | \leq 1.
\end{eqnarray*}
The process may also be well-defined if the stability does not hold.

The VAR(1) process is said to be {\it strictly stationary} if $P(\mathbf{Y}_{\ast,i,1}, \ldots, \mathbf{Y}_{\ast,i,t} ) = P(\mathbf{Y}_{\ast,i,\tau}, \ldots, \mathbf{Y}_{\ast,i,t+\tau} )$ for any $\tau \in \mathbb{N}$. In words, the joint distribution (i.e. $\mmu_y$ and $\SSigma_y$) does not change over time. In case the definition of strictly stationary is relaxed: the first two moments do not dependent on $t$, the process is referred to as \textit{stationary}.

The two concepts are related by Proposition 2.1 of \cite{Luet2005}: model (\ref{form.var1}) yields a stationary time series if it is stable.
\begin{example}
In Example \ref{example.feedforward.motif} the eigenvalues of $\mathbf{A}$ are $\lambda_j(\mathbf{A}) = 0$ for $j=1, \ldots, p$. Hence, the process is stable, and therefore stationary.
\end{example}


% {\it Question:}
% \\
% Is stationarity something that should be required when modeling cancer development / progression? In the limit, everything goes to pieces: the cancer causes death of the host organism. Carel: may be relevant in case of a chronic disease or no-pregression-plus-no-remission. Mark: can this not be assessed from the data?


\subsubsection{The MA representation}
Assume $\mathbf{Y}_{\ast, i, t}$ follows a VAR(1) model. Then, assuming the model is stable, the process can be represented by:
\begin{eqnarray*}
\mathbf{Y}_{\ast, i, t} & = & \mmu + \sum_{t_0 = 0}^{\infty} \mathbf{A}^{t_0} \vvarepsilon_{t - t_0}
\\
& = & \mmu + \sum_{t_0 = 0}^{\infty} \PPhi_{t_0} \vvarepsilon_{t - t_0}.
\end{eqnarray*}
This is called the {\it moving average (MA) representation} (or sometimes {\it canonical}, {\it fundamental} or {\it prediction error representation}). The representation expresses $\mathbf{Y}_{\ast, i, t}$ in past and present error (innovations). The $\PPhi_{t_0}$ are referred to as the moving average coefficient matrices. The key motivation for this representation is the (more explicit) relation to the mean and autocovariance of the process.




\subsubsection{The structural VAR model}
The \textit{structural VAR(1) model} is a reformulation of the canonical VAR(1) model (\ref{form.var1}) that explicitly takes into account the contemporaneous relationships of the variates that constitute $\mathbf{Y}_{\ast, i, t}$. The structural VAR(1) model that describes the gene expression data from the time-course experiment is:
\begin{eqnarray} \label{form.GMvar1}
\mathbf{Y}_{\ast, i, t} % \, | \, \mathbf{Y}_{\ast, i,t-1}, \ldots,  \mathbf{Y}_{\ast, i, 1}
& = & \mathbf{B} \mathbf{Y}_{\ast, i, t} + \breve{\mathbf{A}} \mathbf{Y}_{\ast, i, t-1} + \breve{\vvarepsilon}_{\ast, i, t},
\end{eqnarray}
where $\breve{\mathbf{A}}$ a $p \times p$ coefficient matrix, $\mathbf{B}$ a $p \times p$ coefficient matrix with a zero diagonal such that $\mathbf{I}_{p \times p} - \mathbf{B}$ is non-singular, and $\breve{\vvarepsilon}_{\ast,i,t}$ a $p \times 1$ vector with the errors. Throughout it is assumed that $\breve{\vvarepsilon}_{\ast,i,t} \sim \mathcal{N}(\mathbf{0}_{p \times 1}, \breve{\mathbf{\Sigma}}_{\varepsilon})$ and $\mbox{Cov}(\breve{\vvarepsilon}_{\ast,i_1,t_1}, \breve{\vvarepsilon}_{\ast,i_2,t_2}) = \mathbf{0}$ if $t_1 \not=t_2$ or $i_1 \not=i_2$. Thus, for a 3-gene pathway model (\ref{form.GMvar1}) becomes:
\begin{eqnarray*}
\left\{
\begin{array}{ccc}
Y_{1,i,t} & = & b_{12} Y_{2,i,t-1} + b_{13} Y_{3,i,t} + \breve{a}_{11} Y_{1,i,t-1} + \breve{a}_{12} Y_{2,i,t-1} + \breve{a}_{13} Y_{3,i,t-1} + \breve{\varepsilon}_{1,i,t},
\\
Y_{2,i,t} & = & b_{21} Y_{1,i,t-1} + b_{23} Y_{3,i,t} + \breve{a}_{21} Y_{1,i,t-1} + \breve{a}_{22} Y_{2,i,t-1} + \breve{a}_{23} Y_{3,i,t-1} + \breve{\varepsilon}_{2,i,t},
\\
Y_{3,i,t} & = & b_{31} Y_{1,i,t-1} + b_{32} Y_{2,i,t} + \breve{a}_{31} Y_{1,i,t-1} + \breve{a}_{32} Y_{2,i,t-1} + \breve{a}_{33} Y_{3,i,t-1} + \breve{\varepsilon}_{3,i,t},
\end{array}
\right.
\end{eqnarray*}
where e.g. $Y_{1,i,t}$ does not appear in the first equations as $b_{11} = 0$.

The relations between the parameters of the canonical and structural VAR(1) models are:
\begin{eqnarray*}
\vvarepsilon_{\ast, i, t} & = & (\mathbf{I}_{p \times p} - \mathbf{B})^{-1} \breve{\vvarepsilon}_{\ast, i, t},
\\
\mathbf{A} & = & (\mathbf{I}_{p \times p} - \mathbf{B})^{-1} \breve{\mathbf{A}},
\\
\SSigma_{\varepsilon}  & = & (\mathbf{I}_{p \times p} - \mathbf{B})^{-1} \breve{\SSigma}_{\varepsilon}
[(\mathbf{I}_{p \times p} - \mathbf{B})^{-1}]^{\mathrm{T}},
\end{eqnarray*}
as the canonical VAR(1) model (\ref{form.var1}) is obtained through pre-multiplication of the structural VAR(1) model (\ref{form.GMvar1})
by $(\mathbf{I}_{p \times p} - \mathbf{B})^{-1}$.


\subsection{Mean, variance, and autocovariance}
Under the above assumptions the vector of gene expression levels of sample $i$ at time point $t$ follows a multivariate normal distribution: $\mathbf{Y}_{\ast, i,t} \sim \mathcal{N}(\mmu_y, \mathbf{\Sigma}_{y})$. The mean of $\mathbf{Y}_{\ast,i,t}$ is:
\begin{eqnarray*}
E(\mathbf{Y}_{\ast,i,t} \, | \, \mathbf{Y}_{\ast,i,t-1}, \ldots,  \mathbf{Y}_{\ast,i,1}) & = & E(\nnu) + \mathbf{A} E\big( \mathbf{Y}_{\ast,i,t-1}\big) + E\big(\vvarepsilon_{\ast,i,t}\big)
\\
& = & \nnu + \mathbf{A}  E\big( \nnu + \mathbf{A} \mathbf{Y}_{\ast,i,t-2}  + \vvarepsilon_{\ast,i,t-1} \big)
\\
& = & \nnu + \mathbf{A} \nnu + \mathbf{A}^2 E\big( \nnu + \mathbf{A} \mathbf{Y}_{\ast,i,t-3}  + \vvarepsilon_{\ast,i,t-2}  \big)
\\
& = & \ldots
\\
& = & \big( \mathbf{I}_{p \times p} + \mathbf{A} + \mathbf{A}^2 + \ldots + \mathbf{A}^{\mathcal{T}-1} \big) \nnu.
\end{eqnarray*}
If $\mathcal{T} \rightarrow \infty$, the mean $\mmu_y$ converges to $\big( \mathbf{I}_{p \times p} - \mathbf{A}\big)^{-1} \nnu$. This assumes that all eigenvalues of $\mathbf{A}$ are smaller than 1 in an absolute sense.

The variance of the process is a little bit more elaborate. Note that from the definition of the model (\ref{form.var1}) it is clear that $\mathbf{\Sigma}_{y}$ satisfies:
\begin{eqnarray} \label{form.lyapunov}
\mathbf{\Sigma}_{y} & = & \mathbf{A} \mathbf{\Sigma}_{y} \mathbf{A}^{\mathrm{T}} + \mathbf{\Sigma}_{\varepsilon}.
\end{eqnarray}
This equation is known as the discrete Lyapunov equation. An analytic solution (solving for $\mathbf{\Sigma}_{y}$) of this equation exists. To arrive at the solution
apply the $\mbox{vec}(\cdot)$ operator, defined by
\begin{eqnarray*}
\mbox{vec}(\mathbf{A})
& = & \mbox{vec} \left( \left(
\begin{array}{cccc}
a_{11} & a_{12} & \ldots & a_{1p}
\\
a_{21} & a_{22} & \ldots & a_{2p}
\\
\vdots & \vdots & \ddots & \vdots
\\
a_{p1} & a_{p2} & \ldots & a_{pp}
\end{array}
\right) \right)
\, \, \, = \, \, \,
(a_{11}, \ldots, a_{p1}, a_{12}, \ldots, a_{p2}, \ldots, a_{1p}, \ldots, a_{pp})^{\mathrm{T}},
\end{eqnarray*}
to both sides of Equation (\ref{form.lyapunov}):
\begin{eqnarray*}
\mbox{vec}(\mathbf{\Sigma}_{\varepsilon}) & = &
\mbox{vec}(\mathbf{\Sigma}_{y}) - \mbox{vec}(\mathbf{A} \mathbf{\Sigma}_{y} \mathbf{A}^{\mathrm{T}})
\\
& = &
\mbox{vec}(\mathbf{\Sigma}_{y}) - (\mathbf{A} \otimes \mathbf{A} )
\mbox{vec}(\mathbf{\Sigma}_{y})
\\
& = & (\mathbf{I}_{p^2 \times p^2} - \mathbf{A} \otimes \mathbf{A} )
\mbox{vec}(\mathbf{\Sigma}_{y}),
\end{eqnarray*}
where the identity $\mbox{vec}( \mathbf{A} \mathbf{B} \mathbf{C}) = (\mathbf{C}^{\mathrm{T}} \otimes \mathbf{A}) \mbox{vec}( \mathbf{B})$ is used. Thus:
\begin{eqnarray*}
\mbox{vec}(\mathbf{\Sigma}_{y}) & = & (\mathbf{I}_{p^2 \times p^2} - \mathbf{A} \otimes \mathbf{A} )^{-1} \, \mbox{vec}(\mathbf{\Sigma}_{\varepsilon}),
\end{eqnarray*}
presuming the inverse exists. With respect to this existence note that {\it i)} the eigenvalues of $\mathbf{A} \otimes \mathbf{A}$ are given by $\lambda_{j_1}(\mathbf{A}) \lambda_{j_2}(\mathbf{A})$ for $j_1, j_2=1,\ldots,p$, and {\it ii)} the eigenvalues of $\mathbf{I}_{p^2 \times p^2} - \mathbf{A} \otimes \mathbf{A}$ are then of the form $1 - \lambda_{j_1}(\mathbf{A}) \lambda_{j_2}(\mathbf{A})$. Consequently, the inverse (and thus the solution) above exists if for any eigenvalue of $\mathbf{A}$ its reciprocal is not an eigenvalue of $\mathbf{A}$.
% and $\lambda_{j_1}(\mathbf{A}) \lambda_{j_2}(\mathbf{A}) \not= 1$ for any combination of $j_1$ and $j_2$.
\\
\\
A well-known alternative solution of the discrete Lyapunov equation (\ref{form.lyapunov}) is:
\begin{eqnarray*}
\mathbf{\Sigma}_{y} & = & \sum_{k=0}^{\infty} \mathbf{A}^k \mathbf{\Sigma}_{\varepsilon} (\mathbf{A}^{\mathrm{T}})^k.
\end{eqnarray*}
To see this is indeed a solution of the discrete Lyapunov equation, simply substitute it:
\begin{eqnarray*}
\sum_{k=0}^{\infty} \mathbf{A}^k \mathbf{\Sigma}_{\varepsilon} (\mathbf{A}^{\mathrm{T}})^k & = & \mathbf{A} \Big( \sum_{k=0}^{\infty} \mathbf{A}^k \mathbf{\Sigma}_{\varepsilon} (\mathbf{A}^{\mathrm{T}})^k \Big) \mathbf{A}^{\mathrm{T}} + \mathbf{\Sigma}_{\varepsilon}
\\
& = & \sum_{k=1}^{\infty} \mathbf{A}^k \mathbf{\Sigma}_{\varepsilon} (\mathbf{A}^{\mathrm{T}})^k + \mathbf{\Sigma}_{\varepsilon} \, \, \, =
\, \, \, \sum_{k=0}^{\infty} \mathbf{A}^k \mathbf{\Sigma}_{\varepsilon} (\mathbf{A}^{\mathrm{T}})^k.
\end{eqnarray*}
Furthermore, the positive-definiteness of $\mathbf{\Sigma}_{\varepsilon}$ warrants the positive-definiteness of $\mathbf{\Sigma}_{y}$ (this follows directly from iterative application of results from \citealt{Har2008}). \reminder{Quote proposition number from Harville.}

We now investigate the convergence of this series solution. Hereto diagonalize $\mathbf{A}$ by means of its eigen-decomposition $\mathbf{A} = \mathbf{V}_a \mathbf{D}_a \mathbf{V}^{-1}_a$. The discrete Lyapunov equation then becomes:
\begin{eqnarray*}
\mathbf{\Sigma}_{y} & = & \mathbf{V}_a \mathbf{D}_a \mathbf{V}^{-1}_a \mathbf{\Sigma}_{y} ( \mathbf{V}_a^{-1})^{\mathrm{T}} \mathbf{D}_a \mathbf{V}^{\mathrm{T}}_a  + \mathbf{\Sigma}_{\varepsilon}.
\end{eqnarray*}
Or, reformulated:
\begin{eqnarray*}
\tilde{\mathbf{\Sigma}}_{y} & = & \mathbf{D}_a  \tilde{\mathbf{\Sigma}}_{y} \mathbf{D}_a  + \tilde{\mathbf{\Sigma}}_{\varepsilon},
\end{eqnarray*}
where $\tilde{\mathbf{\Sigma}}_{y}  = \mathbf{V}_a^{-1}  \mathbf{\Sigma}_{y} (\mathbf{V}_a^{-1})^{\mathrm{T}}$ and $\tilde{\mathbf{\Sigma}}_{\varepsilon} = \mathbf{V}_a^{-1}  \mathbf{\Sigma}_{\varepsilon} (\mathbf{V}_a^{-1})^{\mathrm{T}}$. The solution of this reformulated discrete Lyapunov equation is:
\begin{eqnarray*}
\tilde{\mathbf{\Sigma}}_{y} & = & \sum_{k=0}^{\infty} \mathbf{D}_a^k \tilde{\mathbf{\Sigma}}_{\varepsilon} \mathbf{D}_a^k.
\end{eqnarray*}
Or, in terms of the elements of the matrices:
\begin{eqnarray*}
(\tilde{\mathbf{\Sigma}}_{y})_{j_1, j_2} & = & \sum_{k=0}^{\infty} (\mathbf{D}_a^k \tilde{\mathbf{\Sigma}}_{\varepsilon} \mathbf{D}_a^k)_{j_1, j_2}
\\
&  = & (\tilde{\mathbf{\Sigma}}_{\varepsilon})_{j_1, j_2} \sum_{k=0}^{\infty} [(\mathbf{D}_a)_{j_1, j_1}  (\mathbf{D}_a)_{j_2, j_2}]^k
\\
&  = &  \frac{(\tilde{\mathbf{\Sigma}}_{\varepsilon})_{j_1, j_2}}{ 1 - (\mathbf{D}_a)_{j_1, j_1}  (\mathbf{D}_a)_{j_2, j_2}},
\end{eqnarray*}
where we have used that $\mathbf{D}$ is diagonal. If one defines a $p \times p$ matrix $\mathbf{C}$ with $(\mathbf{C})_{j_1, j_2} = [1 - (\mathbf{D}_a)_{j_1, j_1}  (\mathbf{D}_a)_{j_2, j_2}]^{-1}$, then:
\begin{eqnarray*}
\mathbf{\Sigma}_y & = & \mathbf{V}_a (\tilde{\mathbf{\Sigma}}_{\varepsilon} \circ \mathbf{C}) \mathbf{V}_a^{\mathrm{T}},
\end{eqnarray*}
where the $\circ$ operator indicates the Hadamard product. From the above it should also be clear that $\mathbf{\Sigma}_y$ is well-defined by the series solution if $\lambda_{j_1}(\mathbf{A}) \not= [\lambda_{j_2}(\mathbf{A})]^{-1}$ for any couple $j_1$ and $j_2$.

\begin{example} \textit{Mean and variance of the feedforward motif.}
Revisit Example \ref{example.feedforward.motif}. Clearly, as $\nnu = \mathbf{0}_{p \times 1}$, the mean $\mmu$ also equals $\mathbf{0}_{p \times 1}$. The variance of $\mathbf{Y}_{\ast, i, t}$ is:
\begin{eqnarray*}
\SSigma_{y} & = &
\left(
\begin{array}{rrr}
0.2500 & 0.0000 & 0.0000
\\
0.0000 & 1.8125 & -1.1250
\\
0.0000 & -1.1250 & 5.1381
\end{array}
\right),
\end{eqnarray*}
where numbers have been rounded.
\end{example}
\mbox{ }
\\
\\
Now we have defined the mean and variance of the time series process, the remaining interesting quantity in VAR(1) model is the autocovariance. The {\it autocovariance} is the covariance between the gene expression profile at time point $t$ and that at some other time point. The autocovariance of the VAR(1) process is (for $\tau \geq 0$):
\begin{eqnarray*}
\mathbf{\Gamma}(\tau) & = & \mbox{Cov} \big( \mathbf{Y}_{\ast,i,t}, \mathbf{Y}_{\ast,i,t+\tau} \big)
\, \, \, = \, \, \, E\Big[ \big(\mathbf{Y}_{\ast,i,t} - E(\mathbf{Y}_{\ast,i,t}) \big) \big(\mathbf{Y}_{\ast,i,t+\tau} - E(\mathbf{Y}_{\ast,i,t+\tau}) \big)^{\mathrm{T}} \Big]
% \\
% & = & E\Big[ \Big(\mathbf{Y}_{\ast i}^{(t)} - E(\mathbf{Y}_{\ast i}^{(t)}) \big)
% \\
% & &
% \Big( \sum_{k=0}^{\tau-1} \mathbf{A}^k \nnu + \mathbf{A}^{\tau} \mathbf{Y}_{\ast i}^{(t)} + \sum_{k=0}^{\tau-1} \mathbf{A}^k \vvarepsilon_{\ast i}^{(t+\tau-k)}- E \Big(\sum_{k=0}^{\tau-1} \mathbf{A}^k \nnu + \mathbf{A}^{\tau} \mathbf{Y}_{\ast i}^{(t)} + \sum_{k=0}^{\tau-1} \mathbf{A}^k \vvarepsilon_{\ast i}^{(t+\tau-k)} \Big) \Big)^T \Big]
\\
& = & E\Big[ \Big(\mathbf{Y}_{\ast,i,t)} - E(\mathbf{Y}_{\ast,i,t}) \big) \Big( \mathbf{A}^{\tau} \mathbf{Y}_{\ast,i,t} + \sum_{k=0}^{\tau-1} \mathbf{A}^k \vvarepsilon_{\ast,i,t+\tau-k} - E \big(\mathbf{A}^{\tau} \mathbf{Y}_{\ast,i,t} \big) \Big)^{\mathrm{T}} \Big]
\\
& = & E \Big[ \big(\mathbf{Y}_{\ast,i,t} - E(\mathbf{Y}_{\ast,i,t}) \big) \big( \mathbf{A}^{\tau} \mathbf{Y}_{\ast,i,t} - E(\mathbf{A}^{\tau} \mathbf{Y}_{\ast,i,t}) \big)^{\mathrm{T}} \Big]
\, \, \, = \, \, \, \mbox{Cov} \big( \mathbf{Y}_{\ast,i,t}, \mathbf{Y}_{\ast,i,t} \big) [\mathbf{A}^{\tau}]^{\mathrm{T}}
\\
& = & \SSigma_y [\mathbf{A}^{\tau}]^{\mathrm{T}}.
\end{eqnarray*}
Similarly, for $\tau < 0$:
\begin{eqnarray*}
\mathbf{\Gamma}(\tau) \, \, \, = \, \, \,  \mbox{Cov} \big( \mathbf{Y}_{\ast,i,t-\tau}, \mathbf{Y}_{\ast,i,t} \big) & = & \mathbf{A}^{-\tau} \SSigma_y.
\end{eqnarray*}
In the above it is used that $\mbox{Cov} \big( \vvarepsilon_{\ast,i,t}, \vvarepsilon_{\ast,i,t+\tau} \big) = 0$ if $\tau \not= 0$.

Associated with the autocovariance matrix are the {\it autocorrelation} and {\it crosscorrelation} (or sometimes {\it cross-autocorrelation}). Crosscorrelation is defined as $\mbox{Cor}(Y_{j_1,i,t}, Y_{j_2,i,t+\tau})$ for $j_1 \not= j_2$ and $\tau \in \mathbb{N}$. The crosscorrelation is thus the correlation with lag $\tau$ between the time series of two different genes. The autocorrelation is reserved for the special case where $j_1 = j_2$. In the autocovariance matrix for the VAR(1) process, the diagonal elements are thus proportional to the autocorrelations and the off-diagonal elements to the crosscorrelations.

Finally, another frequently used quantity in times series is the {\it partial autocorrelation}. This is defined as:
\begin{eqnarray*}
\mbox{Cor}(Y_{j,i,t}, Y_{j,i,t+\tau} \, | \, Y_{j,i,t+1}, \ldots, Y_{j,i,t+\tau-1}).
\end{eqnarray*}
Hence, the correlation between the expression levels of a gene at two time points which is not propagated via the intermediate time points. The partial autocorrelation is used to determine the lag of the time series. As we assume the lag to equal one throughout, the partial autocovariance and partial autocorrelation appear to be of little interest here.



\subsection{Estimation}
\subsubsection{Mean and autocovariance}
The process mean is estimated by:
\begin{eqnarray*}
\hat{\mmu}_y & = & \frac{1}{n\mathcal{T}} \sum_{i=1}^n \sum_{t=1}^{\mathcal{T}} \mathbf{Y}_{\ast,i,t},
\end{eqnarray*}
which has the following expectation:
\begin{eqnarray*}
E(\hat{\mmu}_y) & = & \frac{1}{n\mathcal{T}} \sum_{i=1}^n \sum_{t=1}^{\mathcal{T}} E( \mathbf{Y}_{\ast,i,t} ) \, \, \, = \, \, \, \frac{1}{n\mathcal{T}} \sum_{i=1}^n \sum_{t=1}^{\mathcal{T}} \mmu_y
\, \, \, = \, \, \,  \mmu_y.
\end{eqnarray*}
Hence, the estimator for the mean is unbiased.

A natural estimator for the autocovariance (confer \citealt{Broc2006}) would be:
\begin{eqnarray*}
\hat{\mathbf{\Gamma}}(\tau) & = &
\left\{
\begin{array}{rcccl}
\frac{1}{n(\mathcal{T}-\tau)} \sum_{i=1}^n \sum_{t=1}^{\mathcal{T}-\tau} ( \mathbf{Y}_{\ast,i,t} - \hat{\mmu}_y) \big(\mathbf{Y}_{\ast,i,t+\tau} - \hat{\mmu}_y \big)^{\mathrm{T}} & & \mbox{for} & 0 \leq \tau  \leq \mathcal{T}-1,
\\
\frac{1}{n(\mathcal{T}-\tau)} \sum_{i=1}^n \sum_{t=-\tau+1}^{\mathcal{T}} ( \mathbf{Y}_{\ast,i,t} - \hat{\mmu}_y) \big(\mathbf{Y}_{\ast,i,t+\tau} - \hat{\mmu}_y \big)^{\mathrm{T}} & & \mbox{for} & -\mathcal{T}+1 \leq \tau  < 0.
\end{array}
\right.
\end{eqnarray*}
However, as we have centered the data, we consider the following estimator:
\begin{eqnarray*}
\hat{\mathbf{\Gamma}}(\tau) & = &
\left\{
\begin{array}{rcccl}
\frac{1}{n(\mathcal{T}-\tau)} \sum_{i=1}^n \sum_{t=1}^{\mathcal{T}-\tau}  \mathbf{Y}_{\ast,i,t} \mathbf{Y}_{\ast,i,t+\tau}^{\mathrm{T}} & & \mbox{for} & 0 \leq \tau  \leq \mathcal{T}-1,
\\
\frac{1}{n(\mathcal{T}-\tau)} \sum_{i=1}^n \sum_{t=-\tau+1}^{\mathcal{T}} \mathbf{Y}_{\ast,i,t} \mathbf{Y}_{\ast,i,t+\tau}^{\mathrm{T}} & &
\mbox{for} & -\mathcal{T}+1 \leq \tau  < 0.
\end{array}
\right.
\end{eqnarray*}
For the calculation of the expectation of the covariance estimator we thus assume that $\mmu_y=0$ (as we have centered the gene expression levels around zero).
Then, for $\tau \geq 0$, we have:
\begin{eqnarray*}
E[ \hat{\mathbf{\Gamma}}(\tau) ] & = & \frac{1}{n(\mathcal{T}-\tau)} \sum_{i=1}^n \sum_{t=1}^{\mathcal{T}-\tau} E[ \mathbf{Y}_{\ast,i,t} \mathbf{Y}_{\ast,i,t+\tau}^{\mathrm{T}} ]
\, \, \, = \, \, \, \GGamma(\tau) .
\end{eqnarray*}
Again, an unbiased estimator. Note that expectation of the `natural' autocovariance estimator of \cite{Broc2006} is not unbiased. It is however consistent.
\\
\\
\reminder{Mark: When $\mu_y$  is assumed  to be zero, the degrees of freedom also changes. Hence, divide by $n \mathcal{T}$ rather than $n(\mathcal{T}-\tau)$! }

% {\it Question:} Is it reasonable to assume the alternative autocovariance estimator?




\subsubsection{Model parameters $\mathbf{A}$ and $\SSigma_{\varepsilon}$}
We now turn to the estimation of the parameters of model (\ref{form.var1}). This is done by likelihood maximization. It should be noted that the ML estimators for $\mathbf{A}$ coincides with that of the least squares approach. As we have centered (per gene, within each sample) the expression values, we may consider $\nnu = \mathbf{0}$. Thus, $\mathbf{Y}_{\ast,i,t} \, | \, \mathbf{Y}_{\ast,i,t-1}, \ldots,  \mathbf{Y}_{\ast,i,1}  \sim \mathcal{N}( \mathbf{A} \mathbf{Y}_{\ast,i,t}  , \SSigma_{\varepsilon})$. The likelihood is then:
\begin{eqnarray*}
& & \hspace{-1cm} L(\mathbf{Y}; \nnu, \mathbf{A}, \SSigma_{\varepsilon})
\\
& = & \prod_{i=1}^n P(\mathbf{Y}_{\ast,i,\mathcal{T}}, \ldots, \mathbf{Y}_{\ast,i,1})
\, \, \,  = \, \, \, \prod_{i=1}^n
\prod_{t=1}^{\mathcal{T}} P(\mathbf{Y}_{\ast,i,t} \, | \, \mathbf{Y}_{\ast,i,t-1}, \ldots, \mathbf{Y}_{\ast,i,1})
\\
& = & \prod_{i=1}^n \prod_{t=2}^{\mathcal{T}} \frac{1}{(2\pi)^{p/2} | \SSigma_{\varepsilon} |^{1/2} } \exp \big[ - \big( \mathbf{Y}_{\ast,i,t} - \mathbf{A} \mathbf{Y}_{\ast,i,t-1}  \big)^\mathrm{T} \SSigma_{\varepsilon}^{-1} \big(\mathbf{Y}_{\ast,i,t} - \mathbf{A} \mathbf{Y}_{\ast,i,t-1}  \big) \big]
\\
& = & \prod_{i=1}^n \prod_{t=2}^{\mathcal{T}} \frac{1}{(2\pi)^{p/2} | \SSigma_{\varepsilon} |^{1/2} }  \exp \big\{ - \big[ \mathbf{Y}_{\ast,i,t} -
\big( \mathbf{Y}_{\ast,i,t-1}^{\mathrm{T}}  \otimes \mathbf{I}_{p \times p} \big) \mbox{vec}(\mathbf{A})  \big]^{\mathrm{T}}
\\
& & \qquad \qquad \qquad \qquad  \qquad \qquad \qquad \, \SSigma_{\varepsilon}^{-1} \big[ \mathbf{Y}_{\ast,i,t} -
\big( \mathbf{Y}_{\ast,i,t-1}^\mathrm{T}  \otimes \mathbf{I}_{p \times p} \big) \mbox{vec}(\mathbf{A})  \big] \big\}
\end{eqnarray*}
% For the estimation of $\mathbf{A}$, we select only the part of the log-likelihood that involves $\mbox{vec}(\mathbf{A})$:
% \begin{eqnarray*}
% - \frac{1}{2}  \sum_{i=1}^n \sum_{t=1}^T  \big[ \mathbf{Y}_{\ast i}^{(t)} - \nnu -
% \big( \big( \mathbf{Y}_{\ast i}^{(t-1)} \big)^T  \otimes \mathbf{I}_{p \times p} \big) \mbox{vec}(\mathbf{A})  \big]^T
% \SSigma_{\varepsilon}^{-1} \big[ \mathbf{Y}_{\ast i}^{(t)} - \nnu -
% \big( \big( \mathbf{Y}_{\ast i}^{(t-1)} \big)^T  \otimes \mathbf{I}_{p \times p} \big) \mbox{vec}(\mathbf{A})  \big]
% \end{eqnarray*}
Equate the derivative of the log-likelihood with respect to $\mbox{vec}(\mathbf{A})$ to zero:
\begin{eqnarray*}
0 & = & \sum_{i=1}^n \sum_{t=2}^{\mathcal{T}} \big( \mathbf{Y}_{\ast,i,t-1}^{\mathrm{T}}  \otimes \mathbf{I}_{p \times p} \big)^{\mathrm{T}}  \SSigma_{\varepsilon}^{-1}
\big(\mathbf{Y}_{\ast,i,t} - \big( \mathbf{Y}_{\ast,i,t-1}^{\mathrm{T}}  \otimes \mathbf{I}_{p \times p} \big) \mbox{vec}(\mathbf{A}) \big).
% \\
% & = & \sum_{i=1}^n \sum_{t=1}^T \big( \mathbf{Y}_{\ast i}^{(t-1)}   \otimes \mathbf{I}_{p \times p} \big)  \SSigma_{\varepsilon}^{-1}
% \big(\mathbf{Y}_{\ast i}^{(t)} - \nnu - \big( \big( \mathbf{Y}_{\ast i}^{(t-1)} \big)^T  \otimes \mathbf{I}_{p \times p} \big) \mbox{vec}(\mathbf{A}) \big)
% \\
% & = & \sum_{i=1}^n \sum_{t=1}^T \big( \mathbf{Y}_{\ast i}^{(t-1)}   \otimes \mathbf{I}_{p \times p} \big) \big( 1 \otimes \SSigma_{\varepsilon}^{-1})
% \big(\mathbf{Y}_{\ast i}^{(t)} - \nnu - \big( \big( \mathbf{Y}_{\ast i}^{(t-1)} \big)^T  \otimes \mathbf{I}_{p \times p} \big) \mbox{vec}(\mathbf{A}) \big)
% \\
% & = & \sum_{i=1}^n \sum_{t=1}^T \big( \mathbf{Y}_{\ast i}^{(t-1)}  \otimes \SSigma_{\varepsilon}^{-1})
% \big(\mathbf{Y}_{\ast i}^{(t)} - \nnu - \big( \big( \mathbf{Y}_{\ast i}^{(t-1)} \big)^T  \otimes \mathbf{I}_{p \times p} \big) \mbox{vec}(\mathbf{A}) \big)
% \\
% & = & \sum_{i=1}^n \sum_{t=1}^T \big( \mathbf{Y}_{\ast i}^{(t-1)}  \otimes \SSigma_{\varepsilon}^{-1}\big)
% \big(\mathbf{Y}_{\ast i}^{(t)} - \nnu \big) -
% \sum_{i=1}^n \sum_{t=1}^T \big( \mathbf{Y}_{\ast i}^{(t-1)}  \otimes \SSigma_{\varepsilon}^{-1}\big) \big( \big( \mathbf{Y}_{\ast i}^{(t-1)} \big)^T  \otimes \mathbf{I}_{p \times p} \big) % \mbox{vec}(\mathbf{A})
% \\
% & = & \sum_{i=1}^n \sum_{t=1}^T \big( \mathbf{Y}_{\ast i}^{(t-1)}  \otimes \SSigma_{\varepsilon}^{-1}\big)
% \big(\mathbf{Y}_{\ast i}^{(t)} - \nnu \big) -
% \sum_{i=1}^n \sum_{t=1}^T \big( \mathbf{Y}_{\ast i}^{(t-1)} \big( \mathbf{Y}_{\ast i}^{(t-1)} \big)^T \otimes \SSigma_{\varepsilon}^{-1}\big) \mbox{vec}(\mathbf{A})
% \\
% & = & \big( 1 \otimes \SSigma_{\varepsilon}^{-1}\big) \sum_{i=1}^n \sum_{t=1}^T \big( \mathbf{Y}_{\ast i}^{(t-1)}  \otimes \mathbf{I}_{p \times p} \big)
% \big(\mathbf{Y}_{\ast i}^{(t)} - \nnu \big)
% \\
% & & \quad - \, \, \big( 1 \otimes \SSigma_{\varepsilon}^{-1}\big)  \sum_{i=1}^n \sum_{t=1}^T \big( \mathbf{Y}_{\ast i}^{(t-1)} \big( \mathbf{Y}_{\ast i}^{(t-1)} \big)^T \otimes \mathbf{I}_{p \times p} \big) \mbox{vec}(\mathbf{A})
% \\
% \big( 1 \otimes \SSigma_{\varepsilon}^{-1}\big) \sum_{i=1}^n \sum_{t=1}^T \big( \mathbf{Y}_{\ast i}^{(t-1)}  \otimes \mathbf{I}_{p \times p} \big)
% \big(\mathbf{Y}_{\ast i}^{(t)} - \nnu \big)
% & = & \big( 1 \otimes \SSigma_{\varepsilon}^{-1}\big)  \big( \sum_{i=1}^n \sum_{t=1}^T \mathbf{Y}_{\ast i}^{(t-1)} \big( \mathbf{Y}_{\ast i}^{(t-1)} \big)^T \otimes \mathbf{I}_{p \times p} \big) \mbox{vec}(\mathbf{A}).
\end{eqnarray*}
After some algebraic manipulations this is rewritten to:
\begin{eqnarray*}
0 & = & \sum_{i=1}^n \sum_{t=2}^{\mathcal{T}} \big( \mathbf{Y}_{\ast,i,t-1}  \otimes \SSigma_{\varepsilon}^{-1}\big)
\mathbf{Y}_{\ast,i,t} -
\sum_{i=1}^n \sum_{t=1}^{\mathcal{T}} \big( \mathbf{Y}_{\ast,i,t-1} \mathbf{Y}_{\ast,i,t-1}^{\mathrm{T}} \otimes \SSigma_{\varepsilon}^{-1}\big) \mbox{vec}(\mathbf{A}).
\end{eqnarray*}
Solve this estimation equation to obtain an estimate for $\mathbf{A}$:
\begin{eqnarray*}
\mbox{vec}(\hat{\mathbf{A}}) & = &
\Big(
\sum_{i=1}^n \sum_{t=2}^{\mathcal{T}} \mathbf{Y}_{\ast,i,t-1} \mathbf{Y}_{\ast,i,t-1}^{\mathrm{T}} \otimes \mathbf{I}_{p \times p}
\Big)^{-1}
\sum_{i=1}^n \sum_{t=2}^{\mathcal{T}} \big( \mathbf{Y}_{\ast,i,t-1}  \otimes \mathbf{I}_{p \times p} \big)
\big(\mathbf{Y}_{\ast,i,t}\big).
\end{eqnarray*}
Notice that this expression can be formulated differently:
\begin{eqnarray*}
\hat{\mathbf{A}} & = &  \Big[ \frac{1}{n(\mathcal{T}-1)} \sum_{i=1}^n \sum_{t=1}^{\mathcal{T}-1}  \mathbf{Y}_{\ast,i,t+1} \mathbf{Y}_{\ast,i,t}^{\mathrm{T}} \Big] \Big[ \frac{1}{n({\mathcal{T}}-1)} \sum_{i=1}^n \sum_{t=1}^{{\mathcal{T}}-1}  \mathbf{Y}_{\ast,i,t} \mathbf{Y}_{\ast,i,t}^{\mathrm{T}} \Big]^{-1}
\end{eqnarray*}
This is a more convenient expression when dealing with high-dimensional data.

Finally, the estimator for $\SSigma_{\varepsilon}$ is given by:
\begin{eqnarray*}
\hat{\SSigma}_{\varepsilon} & = & \frac{1}{n \mathcal{T}}  \sum_{i=1}^n \sum_{t=2}^{\mathcal{T}}  \big[ \mathbf{Y}_{\ast,i,t} - \big( \mathbf{Y}_{\ast, i,t-1}^{\mathrm{T}}  \otimes \mathbf{I}_{p \times p} \big) \mbox{vec}(\mathbf{A})  \big] \big[ \mathbf{Y}_{\ast,i,t} - \big( \mathbf{Y}_{\ast,i,t-1}^{\mathrm{T}}  \otimes \mathbf{I}_{p \times p} \big) \mbox{vec}(\mathbf{A})  \big]^{\mathrm{T}}
\\
& = & \frac{1}{n \mathcal{T}}  \sum_{i=1}^n \sum_{t=2}^{\mathcal{T}}  \big[ \mathbf{Y}_{\ast,i,t} - \mathbf{A} \mathbf{Y}_{\ast,i,t-1}  \big] \big[ \mathbf{Y}_{\ast,i,t} - \mathbf{A} \mathbf{Y}_{\ast,i,t-1}  \big]^{\mathrm{T}}.
\end{eqnarray*}
Thus, $\SSigma_{\varepsilon}$ is simply estimated by the sample residual covariance matrix.



\subsubsection{Bias of the OLS/ML estimator of $\mathbf{A}$}
For small $\mathcal{T}$, exactly the case we usually encounter, the OLS and ML estimator of $\mathbf{A}$, as derived above, are biased. An explicit expression of the bias of the `natural estimator' of $\mathbf{A}$ in the VAR(1) model with $n=1$ has been derived by \cite{Nich1988}. A sketch of their  approach follows. Define:
\begin{eqnarray*}
\hat{\mathbf{\Gamma}}_{\mathcal{T}} (\tau) & = &  \frac{1}{n(\mathcal{T}-\tau)} \sum_{i=1}^n \sum_{t=1}^{\mathcal{T}-\tau}  \mathbf{Y}_{\ast,i,t} \mathbf{Y}_{\ast,i,t+\tau}^{\mathrm{T}},
\end{eqnarray*}
where $\tau = -1$ or $\tau = 0$. Furthermore, define the random variables:
\begin{eqnarray*}
\mathbf{P}_{\mathcal{T}} & = & \big[ \hat{\mathbf{\Gamma}}_{\mathcal{T}}(-1) - \mathbf{\Gamma}(-1) \big] [\mathbf{\Gamma}(0)]^{-1}
\\
\mathbf{Q}_{\mathcal{T}} & = & \big[ \hat{\mathbf{\Gamma}}_{\mathcal{T}}(0) - \mathbf{\Gamma}(0) \big] [\mathbf{\Gamma}(0)]^{-1} \, \, \, = \, \, \,
\hat{\mathbf{\Gamma}}_{\mathcal{T}}(0) [\mathbf{\Gamma}(0)]^{-1} - \mathbf{I}_{p \times p}.
\end{eqnarray*}
The estimator of $\mathbf{A}$ can then be written as:
\begin{eqnarray*}
\hat{\mathbf{A}}_{\mathcal{T}} & = &  \hat{\mathbf{\Gamma}}_{\mathcal{T}}(-1) \big[ \hat{\mathbf{\Gamma}}_{\mathcal{T}}(0) ]^{-1}
\\
& = & \hat{\mathbf{\Gamma}}_{\mathcal{T}}(-1) \big[ \hat{\mathbf{\Gamma}}_{\mathcal{T}}(0) ]^{-1}  +  \mathbf{A} \mathbf{\Gamma}(0) [\hat{\mathbf{\Gamma}}_{\mathcal{T}}(0)]^{-1}   - \mathbf{\Gamma}(-1) [\hat{\mathbf{\Gamma}}_{\mathcal{T}}(0)]^{-1}
\\
& = & \hat{\mathbf{\Gamma}}_{\mathcal{T}}(-1) [\mathbf{\Gamma}(0)]^{-1}  \mathbf{\Gamma}(0) \big[ \hat{\mathbf{\Gamma}}_{\mathcal{T}}(0) ]^{-1} +  \mathbf{A} (\mathbf{I}_{p \times p} + \mathbf{Q}_{\mathcal{T}})^{-1}  - \mathbf{\Gamma}(-1) [\mathbf{\Gamma}(0)]^{-1} \mathbf{\Gamma}(0)  [\hat{\mathbf{\Gamma}}_{\mathcal{T}}(0)]^{-1}
\\
& = & \{ \hat{\mathbf{\Gamma}}_{\mathcal{T}}(-1) [\mathbf{\Gamma}(0)]^{-1}   - \mathbf{\Gamma}(-1) [\mathbf{\Gamma}(0)]^{-1}  \}
\mathbf{\Gamma}(0)  \big[ \hat{\mathbf{\Gamma}}_{\mathcal{T}}(0) \big]^{-1} +  \mathbf{A} (\mathbf{I}_{p \times p} + \mathbf{Q}_{\mathcal{T}})^{-1}
\\
& = & (\mathbf{A} + \mathbf{P}_{\mathcal{T}}) (\mathbf{I}_{p \times p} + \mathbf{Q}_{\mathcal{T}})^{-1}
\\
& = & (\mathbf{A} + \mathbf{P}_{\mathcal{T}} )  \sum_{k=0}^{\infty} (-1)^k \mathbf{Q}_{\mathcal{T}}^k
\\
& = &  \mathbf{A} + \mathbf{P}_{\mathcal{T}} - \mathbf{A} \mathbf{Q}_{\mathcal{T}}   - \mathbf{P}_{\mathcal{T}} \mathbf{Q}_{\mathcal{T}}  + \mathbf{A} \mathbf{Q}_{\mathcal{T}}^2   + \mathbf{P}_{\mathcal{T}} \mathbf{Q}_{\mathcal{T}}^2  + \ldots
\end{eqnarray*}
Now take the expectation on both sides:
\begin{eqnarray*}
E( \hat{\mathbf{A}}_{\mathcal{T}} ) & = &  \mathbf{A} + E(\mathbf{P}_{\mathcal{T}}) - E(\mathbf{A} \mathbf{Q}_{\mathcal{T}} )  - E(\mathbf{P}_{\mathcal{T}} \mathbf{Q}_{\mathcal{T}})  + E(\mathbf{A} \mathbf{Q}_{\mathcal{T}}^2)   + E(\mathbf{P}_{\mathcal{T}} \mathbf{Q}_{\mathcal{T}}^2)  + \ldots
\\
& = &  \mathbf{A} + E(\mathbf{P}_{\mathcal{T}} \mathbf{Q}_{\mathcal{T}})  + E(\mathbf{A} \mathbf{Q}_{\mathcal{T}}^2)   + E(\mathbf{P}_{\mathcal{T}} \mathbf{Q}_{\mathcal{T}}^2)  + \ldots
\end{eqnarray*}
The remaining expectation are not (necessarily) zero, hence the OLS / ML estimator of $\mathbf{A}$ is biased \citep{Nich1988}. However, it is consistent \citep{Luet2005}.

\subsubsection{Estimation with constraints on $\mathbf{A}$}
Add constraints. In particular, the sign of the coefficients may be known. Hence, work out details.


\subsubsection{Ridge estimation}
The ridge estimator for $\mathbf{A}$ is readily obtained by subtracting the term $\frac{1}{2} \mbox{tr}[\mbox{vec}(\mathbf{A})^\mathrm{T} \LLambda \mbox{vec}(\mathbf{A})]$ from the log-likelihood. In case $\LLambda = \lambda \mathbf{I}_{p^2 \times p^2}$ the penalty reduces to $\frac{\lambda}{2} \mbox{tr}[\mbox{vec}(\mathbf{A})^\mathrm{T} \LLambda \mbox{vec}(\mathbf{A})] = \frac{\lambda}{2} \| \mathbf{A} \|_2^2 = \frac{\lambda}{2} \sum_{j_1, j_2 = 1}^p (\mathbf{A})_{j_1, j_2}^2$.
The ridge estimator thus becomes:
\begin{eqnarray*}
\mbox{vec}[\hat{\mathbf{A}}(\lambda)] & = &
\Big(\lambda \mathbf{I}_{p^2 \times p^2} +
\sum_{i=1}^n \sum_{t=2}^{\mathcal{T}} \mathbf{Y}_{\ast,i,t-1} \mathbf{Y}_{\ast,i,t-1}^{\mathrm{T}} \otimes \mathbf{I}_{p \times p}
\Big)^{-1}
\sum_{i=1}^n \sum_{t=2}^{\mathcal{T}} \big( \mathbf{Y}_{\ast,i,t-1}  \otimes \mathbf{I}_{p \times p} \big)
\big(\mathbf{Y}_{\ast,i,t} - \nnu \big).
\end{eqnarray*}
Or, when we use the more efficient formulation of the estimator if $\mathbf{A}$ and write $\LLambda = \lambda \mathbf{I}_{p \times p}$:
\begin{eqnarray} \label{form.ridge.estimator}
\hat{\mathbf{A}}(\lambda) & = &  \Big( \frac{1}{n(\mathcal{T}-1)} \sum_{i=1}^n \sum_{t=1}^{\mathcal{T}-1}  \mathbf{Y}_{\ast,i,t+1} \mathbf{Y}_{\ast,i,t}^{\mathrm{T}} \Big) \Big( \lambda \mathbf{I}_{p \times p} +  \frac{1}{n({\mathcal{T}}-1)} \sum_{i=1}^n \sum_{t=1}^{{\mathcal{T}}-1}  \mathbf{Y}_{\ast,i,t} \mathbf{Y}_{\ast,i,t}^{\mathrm{T}} \Big)^{-1}.
\end{eqnarray}
In any case, ridge estimation in the VAR(1) model is analogous to that in the standard linear regression model.
\\
\\
Let us try to understand the effect of ridge penalty on the estimation of $\mathbf{A}$ within the VAR(1) model. As the ridge estimator is the product of a lag one autocovariance estimate and the inverse of a biased covariance estimat, consider the related object:
\begin{eqnarray*}
\mathbf{A}(\lambda) & = & \GGamma(-1) [\GGamma(0) + \lambda \mathbf{I}_{p \times p}]^{-1}
\\
& = & \mathbf{A} \SSigma_y (\SSigma_y + \lambda \mathbf{I}_{p \times p})^{-1}
\end{eqnarray*}
Define a new stochastic process by convoluting the VAR(1) process with white noise:
\begin{eqnarray} \label{form.var1.modified}
\mathbf{Z}_{\ast, i, t}  & = & \mathbf{Y}_{\ast, i, t} + \ddelta_{\ast, i, t},
\end{eqnarray}
with $\mathbf{Y}_{\ast, i, t}$ as in the original process (\ref{form.var1}) and $\ddelta_{\ast, i, t} \sim \mathcal{N}(\mathbf{0}_{p \times 1}, \lambda \mathbf{I}_{p \times p})$. The variance of this process is then: $\SSigma_y + \lambda \mathbf{I}_{p \times p}$ and $\mathbf{Z}_{\ast, i, t}$ has the same autocovariance as $\mathbf{Y}_{\ast, i, t}$. $\mathbf{A}(\lambda)$ is then the product of the autocovariance with lag -1 and inverse of the variance the subprocess $\mathbf{Z}_{\ast, i, t}$. Inclusion of the ridge penalty could be thought of as adding noise to the process.

Note that this `adding noise' to the process is {\it not} equivalent to replacing $\SSigma_{\varepsilon}$ by  $\SSigma_{\varepsilon} + \lambda \mathbf{I}_{p \times p}$. This would amount to redefining the VAR(1) model to:
\begin{eqnarray*}
\mathbf{Y}_{\ast, i, t} & = & \mathbf{A} \mathbf{Y}_{\ast, i, t-1} +
\varepsilon_{\ast, i, t} + \ddelta_{\ast, i, t}.
\end{eqnarray*}
In this model the $\ddelta_{\ast, i, t}$ are part of the innovations and propagated, which is not the case in the convolution. Among others this yields a covariance matrix unequal to $\SSigma_y + \lambda \mathbf{I}_{p \times p}$. To see this consider the discrete Lyapunov equation modified in accordance with this model:
\begin{eqnarray*}
\mathbf{\Sigma}_{y}(\lambda) & = & \mathbf{A} \mathbf{\Sigma}_{y} (\lambda) \mathbf{A}^{\mathrm{T}} + \mathbf{\Sigma}_{\varepsilon}  + \lambda \mathbf{I}_{p \times p}.
\end{eqnarray*}
This thus implies that
\begin{eqnarray*}
\mathbf{\Sigma}_{y}(\lambda) & = & \sum_{k=0}^{\infty} \mathbf{A}^k [ \mathbf{\Sigma}_{\varepsilon} + \lambda \mathbf{I}_{p \times p}
] (\mathbf{A}^{\mathrm{T}})^k
\\
& = & \sum_{k=0}^{\infty} \mathbf{A}^k \mathbf{\Sigma}_{\varepsilon} (\mathbf{A}^{\mathrm{T}})^k
 +
\lambda \sum_{k=0}^{\infty} \mathbf{A}^k (\mathbf{A}^{\mathrm{T}})^k
\\
& = & \SSigma_y + \lambda \mathbf{I}_{p \times p} + \lambda \sum_{k=1}^{\infty} \mathbf{A}^k (\mathbf{A}^{\mathrm{T}})^k,
\end{eqnarray*}
which is unequal to $\SSigma_y + \lambda \mathbf{I}_{p \times p}$.
\\
\\
Alternatively, define the
\begin{eqnarray*}
\left(
\begin{array}{l}
\mathbf{Z}_{\ast, i, t}
\\
\mathbf{u}_{\ast, i, t}
\end{array}
\right)
& = &
\left(
\begin{array}{r}
\mathbf{Y}_{\ast, i, t} + \ddelta_{\ast, i, t}
\\
\mathbf{A} \, \ddelta_{\ast, i, t}
\end{array}
\right)
\, \,\, \sim \mathcal{N}
\left(
\left(
\begin{array}{r}
\mathbf{0}_{p \times 1}
\\
\mathbf{0}_{p \times 1}
\end{array}
\right)
,
\left(
\begin{array}{cc}
\SSigma_y + \lambda \mathbf{I}_{p \times p}   & \lambda \mathbf{A}
\\
\lambda \mathbf{A}^{\mathrm{T}} &  \lambda \mathbf{A} \mathbf{A}^{\mathrm{T}}
\end{array}
\right)
\right).
\end{eqnarray*}
Then:
\begin{eqnarray*}
\mbox{Var}(\mathbf{u}_{\ast, i, t} \, | \, \mathbf{Z}_{\ast, i, t})
& = & \mathbf{A} \mathbf{A}^{\mathrm{T}} - \lambda \mathbf{A} (\SSigma_y + \lambda \mathbf{I}_{p \times p})^{-1} \mathbf{A}^{\mathrm{T}} \, \, \, = \, \, \, \mathbf{A}(\lambda) \mathbf{A}^{\mathrm{T}}.
\end{eqnarray*}
The last equality follows from:
\begin{eqnarray*}
\mathbf{A}(\lambda) & = & \GGamma(-1) [\GGamma(0) + \lambda \mathbf{I}_{p \times p}]^{-1}
\\
& = & \mathbf{A} \SSigma_y (\SSigma_y + \lambda \mathbf{I}_{p \times p})^{-1}
\\
& = & \mathbf{A} (\SSigma_y + \lambda \mathbf{I}_{p \times p})  (\SSigma_y + \lambda \mathbf{I}_{p \times p})^{-1} -
\lambda \mathbf{A}  (\SSigma_y + \lambda \mathbf{I}_{p \times p})^{-1}
\\
& = & \mathbf{A} - \lambda \mathbf{A} (\SSigma_y + \lambda \mathbf{I}_{p \times p})^{-1}.
\end{eqnarray*}
Post-multiplying with $\mathbf{A}^{\mathrm{T}}$ gives the desired equality.\reminder{What is the point?}

The above also suggests that $\mathbf{A}(\lambda) \mathbf{A}^{\mathrm{T}}$ is symmetric. To see whether this is the case, note that:
\begin{eqnarray*}
\mathbf{A}(\lambda) \SSigma_y \mathbf{A}^{\mathrm{T}} - \mathbf{A} \SSigma_y \mathbf{A}^{\mathrm{T}} & = & \mathbf{A} [ \SSigma_y (\SSigma_y + \lambda \mathbf{I}_{p \times p})^{-1} \SSigma_y -  \SSigma_y] \mathbf{A}^{\mathrm{T}}
\\
& = & \mathbf{A} [\lambda (\SSigma_y + \lambda \mathbf{I}_{p \times p})^{-1} \SSigma_y ] \mathbf{A}^{\mathrm{T}}
\\
& = & \lambda [ \mathbf{A}(\lambda) \mathbf{A}^{\mathrm{T}} ]^{\mathrm{T}}.
\end{eqnarray*}
Then, substition of this results and the previous yields:
\begin{eqnarray*}
\lambda  \mathbf{A}(\lambda) \mathbf{A}^{\mathrm{T}} -
\lambda [ \mathbf{A}(\lambda) \mathbf{A}^{\mathrm{T}} ]^{\mathrm{T}} & = &
\mathbf{0}.
\end{eqnarray*}
Indeed, $\mathbf{A}(\lambda) \mathbf{A}^{\mathrm{T}}$ is symmetric.


% Solve the discrete Lyapunov equation for $\mathbf{A}$ yields:
%\begin{eqnarray*}
% \mathbf{A} & = & (\mathbf{\Sigma}_{y} - \mathbf{\Sigma}_{\varepsilon})^{1/2} \mathbf{\Sigma}_{y}^{-1/2}
% \\
% & = & (\mathbf{\Sigma}_{y} - \mathbf{\Sigma}_{\varepsilon})^{1/2} \mathbf{\Sigma}_{y}^{1/2} [\GGamma(0)]^{-1}.
% \end{eqnarray*}
% Is then:
% \begin{eqnarray*}
% \GGamma(-1) & = & (\mathbf{\Sigma}_{y} - \mathbf{\Sigma}_{\varepsilon})^{1/2} (\mathbf{\Sigma}_{y})^{1/2}?
% \end{eqnarray*}



% Analogous to previous calculations we can now express the series solution of the this modified covariance matrix in terms of a product of three matrices. \reminder{What does this give us?}


% From the estimator (\ref{form.ridge.estimator}) shows that the ridge penalty leads to the use of a biased estimate of the covariance matrix $\SSigma_y$ in the estimation of $\mathbf{A}$. Let us have a closer look at this. Define: $\tilde{\SSigma}_y(\lambda) = \lambda \mathbf{I}_{p \times p} + \SSigma_y$. Substitute this ($\SSigma_y = \tilde{\SSigma}_y(\lambda) - \lambda \mathbf{I}_{p \times p}$) into the discrete Lyapunov equation. Thus:


% \begin{figure}[h!]
% \begin{center}
% \includegraphics[angle=270, scale=0.40]{feedforward3gene_ridgePath.ps}
% \end{center}
% \caption{The ridge regularization path for the feedforward example.
% \label{fig.subgroup.origin}}
% \end{figure}


\[
\]
\reminder{ADD RIDGE ESTIMATION OF $\SSigma_{\varepsilon}$}


\subsection{Granger causality}
Granger causality is a concept that refers to whether one variable is useful in predicting another. It exploits the `arrow of time' logic that a cause has to precede the effect. Pursuing this reasoning, if one variable affects another, knowledge of the former should improve the prediction of the latter.

\begin{defin}
Let $\mathbf{Y}_{\ast, i, t}$ follow a VAR(1) process. The random variable $Y_{j_1, i, t}$ is Granger noncausal in the mean (does not Granger cause in the mean) the random variable $Y_{j_2, i, t}$ if
\begin{eqnarray*}
E(Y_{j_2, i, t} \, | \, \mathbf{Y}_{\ast, i, t-1}, \mathbf{Y}_{\ast, i, t-2}, \ldots ) & = &  E(Y_{j_2, i, t} \, | \, \mathbf{Y}_{\ast \setminus j_1, i, t-1}, \mathbf{Y}_{\ast \setminus j1, i, t-2}, \ldots ).
\end{eqnarray*}
That is, the conditional expectation (forecast) of $Y_{j_2, i, t}$ does not improve by  inclusion of past information of the $Y_{j_1, i, t}$.
\end{defin}

\noindent
Analogous definitions (e.g. Granger causality in distribution) replace the conditional expectation by the conditional variance, MSE, or distribution.

For illustration consider the bivariate VAR(1) process defined by:
\begin{eqnarray*}
\left\{
\begin{array}{ccc}
Y_{1,i,t} & = & \nu_1 + a_{11} Y_{1,i,t-1} + a_{12} Y_{2,i,t-1} + \varepsilon_{1,i,t},
\\
Y_{2,i,t} & = & \nu_2 + a_{21} Y_{1,i,t-1} + a_{22} Y_{2,i,t-1} + \varepsilon_{2,i,t},
\end{array}
\right.
\end{eqnarray*}
If $a_{21} = 0$, then $Y_{1, i, t}$ does not Granger cause $Y_{2, i, t}$. If in addition $a_{12} \not=0$, then $Y_{2, i, t}$ Granger causes $Y_{1, i, t}$. Hence, Granger causality is not a commutative property.

Finally, it is important to stress that Granger causality is not true causality. For instance, suppose the true causal structure is given by $Y_{1, i, t} \leftarrow Y_{2, i, t} \rightarrow Y_{3, i, t}$, where $Y_{2, i, t}$ is not observed. Then, usually one concludes that $Y_{1, i, t}$ Granger causes $Y_{3, i, t}$, and vice versa. Hence, indirect effects are not accounted for by Granger causality.




\subsection{VAR(1) and graphical modelling}
We now focus on the relation between the parameters of the VAR(1) model and conditional independence relationships among the variates of the model.

\subsubsection{Contemporaneous conditional dependencies}
First we address the conditional independency relations between the variates of $\mathbf{Y}_{\ast, i, t}$, which are called \textit{contemporaneous CI relations}. Hereto consider the structural VAR(1) model (\ref{form.GMvar1}). The conditional distribution of $\mathbf{Y}_{\ast, i, t}$ given $\mathbf{Y}_{\ast, i, t-1}$ is then:
\begin{eqnarray*}
\mathbf{Y}_{\ast, i, t} \, | \, \mathbf{Y}_{\ast, i, t-1}  & \sim & \mathcal{N} \{ \mathbf{A} \mathbf{Y}_{\ast, i, t-1},  (\mathbf{I}_{p \times p} - \mathbf{B})^{-1} \breve{\SSigma}_{\varepsilon}
[(\mathbf{I}_{p \times p} - \mathbf{B})^{-1}]^{\mathrm{T}} \}.
\end{eqnarray*}
\reminder{Using stuff to prove the well-known but horribly formulated lemma, we can link the elements of $\mathbf{B}$ to the partial correlations.} The conditional independences between nodes at the same time point may thus be obtained from inversion of $\SSigma_{\varepsilon}$, as in the regular Gaussian graphical model.

A second route is provided by Lemma 6.1.2 of \cite{Real1998}, and reformulated here:
\begin{lemma}
Let $\mathbf{Y}_{\ast, i, t}$ follow a multivariate time series model, e.g. VAR(1), with canonical innovations $\vvarepsilon_{\ast, i, t}$. Then:
\begin{eqnarray*}
& & \hspace{-1.0cm}
\mbox{Corr}(\varepsilon_{j_1, i, t}, \varepsilon_{j_2, i, t} \, | \,
\vvarepsilon_{\ast, i, t}, \vvarepsilon_{\ast, i, t-1}, \ldots \setminus
\{ \varepsilon_{j_1, i, t}, \varepsilon_{j_2, i, t} \} )
\\
& = &
\mbox{Corr}(\varepsilon_{j_1, i, t}, \varepsilon_{j_2, i, t} \, | \,
\mathbf{Y}_{\ast, i, t}, \mathbf{Y}_{\ast, i, t-1}, \ldots \setminus
\{ \varepsilon_{j_1, i, t}, \varepsilon_{j_2, i, t} \} )
\\
& = &
\mbox{Corr}(Y_{j_1, i, t}, Y_{j_2, i, t} \, | \,
\mathbf{Y}_{\ast, i, t}, \mathbf{Y}_{\ast, i, t-1}, \ldots \setminus
\{ \varepsilon_{j_1, i, t}, \varepsilon_{j_2, i, t} \} ),
\end{eqnarray*}
for $j_1, j_2 \in \{1, \ldots, p \}$.
\end{lemma}
\noindent
As a direct consequence of this lemma \cite{Real1998} deduces:
\begin{coro}
Let $\mathcal{G}$ be the moral graph (representing the conditional independence relations) of $\mathbf{Y}_{\ast, i, t}, \mathbf{Y}_{\ast, i, t-1}, \ldots$, and $\mathcal{G}_t$ the subgraph of $\mathcal{G}$ related only to $\mathbf{Y}_{\ast, i, t}$ and its conditional independencies. The $\mathcal{G}_t$ is identical to the moral graph of $\vvarepsilon_{\ast, i, t}$.
\end{coro}


Alternatively, consider the covariance between $\mathbf{Y}_{\ast, i, t-1}$ and $\mathbf{Y}_{\ast, i, t-1}$:
\begin{eqnarray*}
\mbox{Cov}
\left(
\begin{array}{l}
\mathbf{Y}_{\ast, i, t-1}
\\
\mathbf{Y}_{\ast, i, t}
\end{array}
\right) & = &
\left(
\begin{array}{rl}
\SSigma_y & \SSigma_y \mathbf{A}^{\mathbf{T}}
\\
\mathbf{A} \SSigma_y & \SSigma_y
\end{array}
\right).
\end{eqnarray*}
Using the analytic expression for the inverse of a $2 \times 2$ block matrix, the corresponding precision matrix equals:
\begin{eqnarray*}
\left(
\begin{array}{rr}
\SSigma_y^{-1} + \mathbf{A}^{\mathrm{T}} (\SSigma_y - \mathbf{A} \SSigma_y \mathbf{A}^{\mathrm{T}})^{-1} \mathbf{A} & - \mathbf{A}^{\mathrm{T}} (\SSigma_y - \mathbf{A} \SSigma_y \mathbf{A}^{\mathrm{T}})^{-1}
\\
-  (\SSigma_y - \mathbf{A} \SSigma_y \mathbf{A}^{\mathrm{T}})^{-1} \mathbf{A} & (\SSigma_y - \mathbf{A} \SSigma_y \mathbf{A}^{\mathrm{T}})^{-1}
\end{array}
\right).
\end{eqnarray*}
As $\SSigma_y$, $\SSigma_{\varepsilon}$ and $\mathbf{A}$ satisfy the Lyapunov equation, we may write $\SSigma_y - \mathbf{A} \SSigma_y \mathbf{A}^{\mathrm{T}} = \SSigma_{\varepsilon}$. Hence, the precision matrix simplifies to:
\begin{eqnarray*}
\left(
\begin{array}{rr}
\SSigma_y^{-1} + \mathbf{A}^{\mathrm{T}} \SSigma_{\varepsilon}^{-1} \mathbf{A} & - \mathbf{A}^{\mathrm{T}} \SSigma_{\varepsilon}^{-1}
\\
-  \SSigma_{\varepsilon}^{-1} \mathbf{A} & \SSigma_{\varepsilon}^{-1}
\end{array}
\right).
\end{eqnarray*}
The bottom-right block of the precision matrix implies that the conditional independencies within $\mathbf{Y}_{\ast, i, t}$ can be read from $\SSigma_{\varepsilon}^{-1}$. Moreover, it suggests that those between $\mathbf{Y}_{\ast, i, t-1}$ and $\mathbf{Y}_{\ast, i, t}$ can be deduced from $\SSigma_{\varepsilon}^{-1} \mathbf{A}$.

Let $\mathbf{Y}_{\ast, i, t}$ follow a VAR(1) process. Then:
$\mathbf{Y}_{\ast, i, t} \, | \, \mathbf{Y}_{\ast, i, t-1}, \, \mathbf{Y}_{\ast, i, t-2}, \ldots = \mathbf{Y}_{\ast, i, t} \, | \, \mathbf{Y}_{\ast, i, t-1} \sim \mathcal{N}( \mathbf{A} \mathbf{Y}_{\ast, i, t-1}, \SSigma_{\varepsilon})$. The density of $\mathbf{Y}_{\ast, i, t} \, | \, \mathbf{Y}_{\ast, i, t-1}$ can then be written as:
\begin{eqnarray*}
& & \hspace{-1.5cm} P(\mathbf{Y}_{\ast, i, t} \, | \, \mathbf{Y}_{\ast, i, t-1})
\\
& \propto & \exp \left[ -(\mathbf{Y}_{\ast, i, t}  - \mathbf{A} \mathbf{Y}_{\ast, i, t-1})^{\mathrm{T}} \, \SSigma_{\varepsilon}^{-1}
\, (\mathbf{Y}_{\ast, i, t}  - \mathbf{A} \mathbf{Y}_{\ast, i, t-1}) / 2 \right]
\\
& = & \exp \Big[ -\frac{1}{2} \sum_{j_1, j_2=1}^p (\mathbf{Y}_{\ast, i, t}  - \mathbf{A} \mathbf{Y}_{\ast, i, t-1})_{j_1, 1} \,  (\SSigma_{\varepsilon}^{-1})_{j_1, j_2} \, (\mathbf{Y}_{\ast, i, t}  - \mathbf{A} \mathbf{Y}_{\ast, i, t-1})_{j_2, 1} \Big]
\\
& = & \exp \Big[ -\frac{1}{2} \sum_{j_1, j_2=1}^p
(\SSigma_{\varepsilon}^{-1})_{j_1, j_2} \, Y_{j_1, i, t} \,
Y_{j_2, i, t} + (\SSigma_{\varepsilon}^{-1})_{j_1, j_2} \,
(\mathbf{A} \mathbf{Y}_{\ast, i, t-1})_{j_1, 1} \,
(\mathbf{A} \mathbf{Y}_{\ast, i, t-1})_{j_2, 1}
\\
&  & \qquad \qquad  - (\SSigma_{\varepsilon}^{-1})_{j_1, j_2} \,
Y_{j_1, i, t} \, (\mathbf{A} \mathbf{Y}_{\ast, i, t-1})_{j_2, 1}
- (\SSigma_{\varepsilon}^{-1})_{j_1, j_2} \, Y_{j_2, i, t} \,
(\mathbf{A} \mathbf{Y}_{\ast, i, t-1})_{j_1, 1} \Big].
\end{eqnarray*}
If $(\SSigma_{\varepsilon}^{-1})_{j_1, j_2} = 0$ for some $j_1, j_2 \in \{ 1, \ldots, p \}$, it is clear that the corresponding term
$Y_{j_1, i, t} Y_{j_2, i, t}$ does not appear in the conditional density above. Hence, the conditional density can be factorized such that $Y_{j_1, i, t}$ and $Y_{j_2, i, t}$ do not occur in the same factor.


\begin{example}
\textit{Illustration of conditional independence of contemporaneous variates.}
\end{example}


\subsubsection{Conditional dependencies across time}
How does this relate to a DAG?



\subsection{Inference on parameters}
How to infer whether elements of $\mathbf{A}$ and (inverse of) $\SSigma_{\varepsilon}$ are zero?

\subsection{Illustration}
Relate to do-calculus of Pearl? Is this related to impulse response analysis? Affect one variable, see what happens downstream? How could the VAR(1) machinery presented here benefit cancer research?


\newpage
\section{Two molecular levels}
\subsection{The VARX(1) model}
The gene expression data from the time-course experiment is modeled by a VAR(1) (first-order vector autoregressive) process with time-varying vector of covariates:
\begin{eqnarray} \label{form.var1.ext}
\mathbf{Y}_{\ast,i,t} % \, | \, \mathbf{Y}_{\ast,i,t-1}, \ldots,  \mathbf{Y}_{\ast,i,1}
& = & \nnu + \mathbf{A} \mathbf{Y}_{\ast,i,t-1}
+ \mathbf{C} \mathbf{X}_{\ast,i,t} + \vvarepsilon_{\ast,i,t},
\end{eqnarray}
where $\nnu$ the $p \times 1$ intercept vector, $\mathbf{A}$ a $p \times p$ coefficient matrix, $\mathbf{C}$ a $p \times q$ coefficient matrix, and $\vvarepsilon_{\ast,i,t+1}$ a $p \times 1$ vector with the errors. Throughout it is assumed that $\vvarepsilon_{\ast,i,t} \sim \mathcal{N}(\mathbf{0}_{p \times 1}, \mathbf{\Sigma}_{\varepsilon})$
and $\mbox{Cov}(\vvarepsilon_{\ast,i_1,t_1}, \vvarepsilon_{\ast,i_2,t_2}) = \mathbf{0}$ if $t_1 \not=t_2$ or $i_1 \not=i_2$.


When the covariates represent the expression of microRNAs, it would perhaps be more realistic to consider a model in which the microRNAs also follow a VAR(1) process. Model (\ref{form.var1.ext}) thus becomes:
\begin{eqnarray*} & \label{form.var1.ext2}
\left\{
\begin{array}{lcrrrrrrr}
\mathbf{Y}_{\ast,i,t} % \, | \, \mathbf{Y}_{\ast,i,t-1}, \ldots,  \mathbf{Y}_{\ast,i,1}, \mathbf{X}_{\ast,i,t}
& = & \nnu & + & \mathbf{A} \mathbf{Y}_{\ast,i,t-1} &
+ & \mathbf{C} \mathbf{X}_{\ast,i,t} & + & \vvarepsilon_{\ast,i,t}^{(y)}
\\
\mathbf{X}_{\ast,i,t} % \, | \, \mathbf{X}_{\ast,i,t-1}, \ldots,  \mathbf{X}_{\ast, i,1}
& = & \ggamma & + &  &
+ & \mathbf{D} \mathbf{X}_{\ast,i,t} & + & \vvarepsilon_{\ast,i,t}^{(x)}
\end{array}
\right.
\end{eqnarray*}
This is similar to Carel's  stuff.


\subsection{Parameter estimation}
As gene expression levels are assumed to be gene-wise centered around zero, it seems reasonable to set $\nnu = \mathbf{0}_{p \times p}$. Model (\ref{form.var1.ext}) reduces to:
\begin{eqnarray*}
\mathbf{Y}_{\ast,i,t} % \, | \, \mathbf{Y}_{\ast,i,t-1}, \ldots,  \mathbf{Y}_{\ast,i,1}
& = & \mathbf{A} \mathbf{Y}_{\ast,i,t-1}
+ \mathbf{C} \mathbf{X}_{\ast,i,t} + \vvarepsilon_{\ast,i,t}.
\end{eqnarray*}
This may be rewritten as:
\begin{eqnarray*}
\mathbf{Y}_{\ast,i,t} & = & ( \mathbf{A} \, | \, \mathbf{C} ) \left(
\begin{array}{l}
\mathbf{Y}_{\ast,i,t-1}
\\
\mathbf{X}_{\ast,i,t}
\end{array} \right) + \vvarepsilon_{\ast,i,t}
\\
& := & ( \mathbf{A} \, | \, \mathbf{C} ) \, \mathbf{Z}  + \vvarepsilon_{\ast,i,t},
\end{eqnarray*}
where $( \mathbf{A} \, | \, \mathbf{C} )$ is a $p \times (p + q)$ dimensional matrix and $\mathbf{Z}$ a $(p+q) \times 1$ dimensional vector.
Vectorizing (that is, applying the $\mbox{vec}(\cdot)$ operator) to both sides of the equation, one obtains:
\begin{eqnarray*}
\mathbf{Y}_{\ast,i,t} & = & \mbox{vec}[( \mathbf{A} \, | \, \mathbf{C} ) \,   \mathbf{Z}]  + \vvarepsilon_{\ast,i,t}
\\
& = & (\mathbf{Z} \otimes \mathbf{I}_{p \times p}) \, \mbox{vec}[( \mathbf{A} \, | \, \mathbf{C} )]  + \vvarepsilon_{\ast,i,t}
\\
& = & [ (\mathbf{Y}_{\ast,i,t-1}^{\mathrm{T}} \, |  \, \mathbf{X}_{\ast,i,t}^{\mathrm{T}}) \otimes \mathbf{I}_{p \times p} ]  \, \mbox{vec}[( \mathbf{A} \, | \, \mathbf{C} )]  + \vvarepsilon_{\ast,i,t},
\end{eqnarray*}
where $[ (\mathbf{Y}_{\ast,i,t-1}^{\mathrm{T}} \, |  \, \mathbf{X}_{\ast,i,t}^{\mathrm{T}}) \otimes \mathbf{I}_{p \times p} ]$  and $\mbox{vec}[( \mathbf{A} \, | \, \mathbf{C} )]$ are of dimensions $p \times (p^2 + p q)$ and $(p^2 + p q) \times 1$. The product of these terms has the same dimension as $Y_{\ast, i, t}$: $p \times 1$ as desired.

The last formulation of model (\ref{form.var1.ext}) is in the form of a standard linear regression model and facilitates  estimation. The log-likelihood based loss function for $\mbox{vec}[( \mathbf{A} \, | \, \mathbf{C} )]$ is:
\begin{eqnarray*}
\sum_{i=1}^n \sum_{t=2}^{\mathcal{T}} \left\{
\mathbf{Y}_{\ast,i,t} - [ (\mathbf{Y}_{\ast,i,t-1}^{\mathrm{T}} \, |  \, \mathbf{X}_{\ast,i,t}^{\mathrm{T}}) \otimes \mathbf{I}_{p \times p} ]  \, \bbeta   \right\}^{\mathrm{T}} \, \SSigma_{\varepsilon}^{-1} \, \left\{ \mathbf{Y}_{\ast,i,t} - [ (\mathbf{Y}_{\ast,i,t-1}^{\mathrm{T}} \, |  \, \mathbf{X}_{\ast,i,t}^{\mathrm{T}}) \otimes \mathbf{I}_{p \times p} ]  \, \bbeta   \right\},
\end{eqnarray*}
where $\bbeta = \mbox{vec}[( \mathbf{A} \, | \, \mathbf{C} )]$.  Differentiate this loss function with respect to $\bbeta$, equate the resulting derivate to zero, and solve for $\bbeta$ to arrive at the OLS estimator. These steps are algebraic analogous to those in the estimation of the parameters of the regular VAR(1) model (\ref{form.var1}). The ML estimator (which coincides with the OLS estimator) is thus:
\begin{eqnarray*}
\hat{\bbeta} & = &
\left(
\sum_{i=1}^n \sum_{t=2}^{\mathcal{T}} \left(
\begin{array}{l}
\mathbf{Y}_{\ast,i,t-1}
\\
\mathbf{X}_{\ast,i,t}
\end{array} \right)
(\mathbf{Y}_{\ast,i,t-1}^{\mathrm{T}} \, |  \, \mathbf{X}_{\ast,i,t}^{\mathrm{T}}) \otimes \mathbf{I}_{p \times p}
\right)^{-1}
\sum_{i=1}^n \sum_{t=2}^{\mathcal{T}} \left( \left(
\begin{array}{l}
\mathbf{Y}_{\ast,i,t-1}
\\
\mathbf{X}_{\ast,i,t}
\end{array} \right)  \otimes \mathbf{I}_{p \times p} \right)
\, \mathbf{Y}_{\ast,i,t}.
\end{eqnarray*}
Notice that this expression can be formulated differently \reminder{to do}:
\begin{eqnarray*}
\hat{\bbeta} & = &  \Big[ \frac{1}{n(\mathcal{T}-1)} \sum_{i=1}^n \sum_{t=1}^{\mathcal{T}-1}  \mathbf{Y}_{\ast,i,t+1} \mathbf{Y}_{\ast,i,t}^{\mathrm{T}} \Big] \Big[ \frac{1}{n({\mathcal{T}}-1)} \sum_{i=1}^n \sum_{t=1}^{{\mathcal{T}}-1}  \mathbf{Y}_{\ast,i,t} \mathbf{Y}_{\ast,i,t}^{\mathrm{T}} \Big]^{-1}
\end{eqnarray*}
This is a more convenient expression when dealing with high-dimensional data.







% \section{Example of simple motif}
% WIKI-page on vector autoregression.
% To research:
% \\
% - Upstream in DAG.
% \\
% - Damping
% \\
% - Feedback (on itself)
% \\
% - Loops
% \\
% - Motifs.


\newpage
\setlength{\bibsep}{2pt}
\bibliographystyle{/home/wessel/Research/Articles/natbib}
\bibliography{/home/wessel/Research/Articles/genomics_literature}
% \bibliographystyle{F:/Wessel/Research/Articles/natbib}
% \bibliography{F:/Wessel/Research/Articles/genomics_literature}
% \bibliographystyle{U:/Wessel/Research/Articles/natbib}
% \bibliography{U:/Wessel/Research/Articles/genomics_literature}




\newpage
\section{Appendix A: Matrix algebra}


\begin{ther} \reminder{Add REF!!!}
\\
Suppose a symmetric matrix can be partitioned as:
\begin{eqnarray*}
\left( \begin{array}{cc} \mathbf{A} & \mathbf{B} \\ \mathbf{B}^T & \mathbf{D} \end{array} \right).
\end{eqnarray*}
Its inverse can than be expressed as:
\begin{eqnarray*}
\left( \begin{array}{cc} \mathbf{A}^{-1} + \mathbf{F} \, \mathbf{E}^{-1} \, \mathbf{F}^T  & - \mathbf{F} \mathbf{E}^{-1}
\\
- \mathbf{E}^{-1} \, \mathbf{F}^T &  \mathbf{E}^{-1}
\end{array}
\right),
\end{eqnarray*}
where $\mathbf{E} = \mathbf{D} - \mathbf{B}^T \, \mathbf{A}^{-1} \, \mathbf{B}$ and
$\mathbf{F} = \mathbf{A}^{-1} \, \mathbf{B}$.
\end{ther}



\begin{ther} ({\it Theorem 13.3.8}, \citealp{Har2008})
\\
Let $\mathbf{T}$ represent an $m \times m$ matrix, $\mathbf{U}$ an $m \times n$ matrix, $\mathbf{V}$ an $n \times m$ matrix, and $\mathbf{W}$ an $n \times n$ matrix. If $\mathbf{T}$ is nonsingular, then:
\begin{eqnarray*}
\left| \begin{array}{cc} \mathbf{T} & \mathbf{U} \\ \mathbf{V} & \mathbf{W} \end{array} \right| & = & |\mathbf{T}| \, | \mathbf{W} - \mathbf{V} \mathbf{T}^{-1} \mathbf{U} |.
\end{eqnarray*}
\end{ther}


\begin{ther} ({\it Theorem 14.2.9}, \citealp{Har2008})
\\
\citep{Har2008} Let $\mathbf{A}$ represent an $n \times n$ matrix, and $\mathbf{P}$ an $n \times m$ matrix. (1) If $\mathbf{A}$
is nonnegative definite, then $\mathbf{P}^T \, \mathbf{A} \, \mathbf{P}$ is nonnegative definite. (2) If $\mathbf{A}$ is nonnegative definite and $\mbox{rank}(\mathbf{P}) < m$, then $\mathbf{P}^T \, \mathbf{A} \, \mathbf{P}$ is positive semidefinite. (3) If $\mathbf{A}$ is nonnegative definite and $\mbox{rank}(\mathbf{P}) = m$, then $\mathbf{P}^T \, \mathbf{A} \, \mathbf{P}$ is positive definite.
\end{ther}

\begin{coro} ({\it Corollary 14.8.6} \citealp{Har2008})
\\
Suppose that a symmetric matrix $\mathbf{A}$ is partitioned as:
\begin{eqnarray*}
\mathbf{A} & = &  \left( \begin{array}{cc} \mathbf{T} & \mathbf{U} \\ \mathbf{U}^T & \mathbf{W} \end{array} \right)
\end{eqnarray*}
(where $\mathbf{T}$ and $\mathbf{W}$ are square). Then, $\mathbf{A}$ is positive definite if and only if $\mathbf{T}$ and the Schur complement $\mathbf{W} - \mathbf{U}^T \, \mathbf{T}^{-1} \, \mathbf{U}$ of $\mathbf{T}$ are both positive definite. Similarly, $\mathbf{A}$ is positive definite if and only if $\mathbf{W}$ and the Schur complement $\mathbf{T} - \mathbf{U} \mathbf{W}^{-1} \mathbf{U}^T$ of $\mathbf{W}$ are both positive definite.
\end{coro}

\begin{coro} ({\it Corollary 18.1.7} \citealp{Har2008})
\\
For any symmetric positive definite matrix $\mathbf{A}$ and for any $n \times n$ symmetric matrix $\mathbf{C}$ such that $\mathbf{C} - \mathbf{A}$ is nonnegative definite,
\begin{eqnarray*}
| \mathbf{C} | & \geq  & | \mathbf{A} |,
\end{eqnarray*}
with equality holding if and only if $\mathbf{C} = \mathbf{A} $.
\end{coro}


\begin{ther} (Weyl's monotonicity theorem; \reminder{Add ref})
\\
If $\mathbf{A}$ and $\mathbf{B}$ are $p \times p$, symmetric matrices, and $\mathbf{B}$ is positive semidefinite, then:
\begin{eqnarray*}
\lambda_j (\mathbf{A}) & \leq & \lambda_j ( \mathbf{A} + \mathbf{B} ),
\end{eqnarray*}
for all $j = 1, \ldots, p$.
\end{ther}


\[
\]

{\bf Exercise 33 to Section 14.8} \citep{Har2008} Let $\mathbf{A}$ represent a symmetric nonnegative definite matrix that has been partitioned as
\begin{eqnarray*}
\mathbf{A} & = &  \left( \begin{array}{cc} \mathbf{T} & \mathbf{U} \\ \mathbf{V} & \mathbf{W} \end{array} \right),
\end{eqnarray*}
where $\mathbf{T}$ (and hence $\mathbf{W}$) is square. Show that $\mathbf{V} \, \mathbf{T}^- \, \mathbf{U}$ and $\mathbf{U} \, \mathbf{W}^- \, \mathbf{V}$ are symmetric and nonnegative definite.
\\
\\
Suppose:
\\
i) $\mathbf{W} - \mathbf{V} \mathbf{T}^{-1} \mathbf{U} \succ 0$ (follows from Corollary 14.8.6).
\\
ii) $\mathbf{V} \mathbf{T}^{-1} \mathbf{U} \succeq 0$
\\
Then, we apply Corollary 18.1.7 with $\mathbf{C} = \mathbf{W}$ and $\mathbf{A} = \mathbf{W} - \mathbf{V} \mathbf{T}^{-1} \mathbf{U}$, and it follows that:
\begin{eqnarray*}
|\mathbf{W} | & \geq & | \mathbf{W} - \mathbf{V} \mathbf{T}^{-1} \mathbf{U} |.
\end{eqnarray*}



\end{document}

\newpage
\subsection{Network construction}
\noindent
Construction of \textit{correlation graph}:
\begin{enumerate}
\item Calculate the correlation between all gene pairs.

\item Draw an edge between two nodes if their pairwise correlation exceeds a certain threshold.
\end{enumerate}
This results in many spurious edges drawn. The corresponding genes exhibit a high correlation but are not biologically related.
\\
\\
Construction of \textit{concentration graph}:
\begin{enumerate}
\item Calculate the correlation matrix of the gene expression data.

\item Invert the correlation matrix to obtain all full partial correlation coefficients.

\item Draw an edge between two nodes if their pairwise correlation exceeds a certain threshold (this may be a significance level).
\end{enumerate}
Some remarks with respect to concentration graphs:
\begin{itemize}
\item The use of partial correlation in the construction of the regulatory network still does not infer causal relationships, but it excludes many of the possibilities (De la Fuente \textit{et al}., 2004).

\item The estimation of the full partial correlation coefficient conditional is problematic when dealing with small sample sizes (Markowetz, Spang, 2007).

\item Simulations conducted by De la Fuente \textit{et al}. (2004) indicate that little is gained when constructing networks using third or higher order partial correlations.

\item If the data are not multivariate normal, then zero partial correlations need not imply conditional independence, but rather conditional uncorrelatedness. However, regardless of their distributional assumptions, zero partial correlations among variates are of interest as long as the relationship between the variables has a strong linear component (Magwene, Kim, 2004).

\item When using full partial correlations all high correlations, which can be attributed to other genes, are filtered out. In addition, it may also draw attention to genes which are only weakly correlated with a gene of interest, but highly related in terms of partial correlations (Markowetz, Spang, 2007).

\item Partial correlations vanish: they provide a strong measure of dependence, and only a weakly criterion of independence. Vice versa,
the correlation coefficient is a weak criterion for dependence, but a strong measure of independence (Markowetz, Spang, 2007).
\end{itemize}
Other methods are described in Markowetz, Spang (2007).
\\
\\
Possible useful R-package: {\tt corpcor}, function: {\tt rebuild.cov}.
\\
\\
The model behind such an analysis may be motivated from the rate equations describing the mRNA concentrations, assuming the system is in steady state. Then, $\mathbf{R} \, \mathbf{X} = \mathbf{A} \, \mathbf{X}$. Allowing for noise, the model can be written as:
\begin{eqnarray*}
P(X_i \, | \, \mathbf{X}, \mathbf{\theta}) & \sim & N(\mathbf{A} \, \mathbf{X}, \sigma^2),
\end{eqnarray*}
where $\mathbf{\theta}$ is a parameter vector.
\\
\reminder{Question: how does $\mathbf{A}$ relate to the partial correlations?}



\subsection{The dynamics of transcription}
The rate equations for the transcriptional process link the reaction rate ($\approx$ change in transcript abundance)
with concentrations of mRNAs, microRNAs, proteins and other molecules and with external influences (De Jong, 2002).
Within this framework regulatory interactions take the form of functional and differential relations between the concentration variables. More specifically, gene regulation is modeled by rate equations expressing the rate of production of a
component of the system as a function of the concentrations of other components:
\begin{eqnarray*}
\underset{\mbox{production rate}}{ \underbrace{ \frac{d \, \mathbf{Z}}{d \, t} } }
& = & \underset{\mbox{transcription}}{ \underbrace{ G(\mathbf{Z}) } } -
\underset{\mbox{degradation}}{ \underbrace{ \mathbf{R} \, \mathbf{Z} } },
\end{eqnarray*}
where $\mathbf{Z}$ denotes the vector of cellular transcript concentrations, $G(\mathbf{Z})$ is a vector of nonlinear functions, and $\mathbf{R}$ is a diagonal matrix.

Close to a steady state the system of nonlinear differential equations may be approximated by a linear system of equations (Xiong \textit{et al}., 2004):
\begin{eqnarray*}
\frac{d \, \mathbf{Y}}{d \, t}  & = & \mathbf{A} \, \mathbf{Y}  - \mathbf{R} \, \mathbf{Y},
\end{eqnarray*}
where $\mathbf{Y}$ is the vector of deviations from $\mathbf{Z}$ in its steady state and $\mathbf{A}$ is the Jacobian matrix of $G(\mathbf{Z})$, e.g., $\partial \, G(\mathbf{Z}) / \partial \, \mathbf{Z}$, measuring the strength of regulatory interactions between genes in the network. At steady state, the time derivative is zero, and we have: $\mathbf{R} \, \mathbf{Y}  =  \mathbf{A} \, \mathbf{Y})$.

A cellular system at steady state can be manipulated by external stimuli, such as a compound that inhibits the activity of a particular protein or changes the expression of a gene via an inducible promoter, and can be determined as the change in gene expression due to this stimulus, for example, by comparing gene expression in the perturbed cellular population against a reference unperturbed population. This approach has the advantage that a non-linear system can generally be approximated as a linear system around a steady state, providing a basis for the linearity assumption.
The model may extended to incorporate external perturbations (Margolin \textit{et al}., 2007):
\begin{eqnarray*}
\frac{d \, \mathbf{Y}}{d \, t}  & = & \mathbf{A} \, \mathbf{Y}  - \mathbf{R} \, \mathbf{Y} + \mathbf{B},
\end{eqnarray*}
where $\mathbf{B}$ is a matrix representing the effect of perturbations on each transcript.
\\
\\
Assuming that only observed variables need to be into account has the biological implication
that gene regulation is assumed to be controlled entirely by these variables (Margolin \textit{et al}., 2007).




\end{document}


\newpage
\newpage
\newpage
\newpage
\subsection{Estimation}
We now turn to the estimation of the parameters of model (\ref{form.var1}). This is done by likelihood maximization. It should be noted that the ML estimators for $\mathbf{A}$ coincides with that of the least squares approach. As $\mathbf{Y}_{\ast i}^{(t)} \, | \, \mathbf{Y}_{\ast i}^{(t-1)}, \ldots,  \mathbf{Y}_{\ast i}^{(1)}  \sim \mathcal{N}( \nnu + \mathbf{A} \mathbf{Y}_{\ast i}^{(t)}  , \SSigma_{\varepsilon})$, the likelihood is given by:
\begin{eqnarray*}
& & \hspace{-1cm} L(\mathbf{Y}; \nnu, \mathbf{A}, \SSigma_{\varepsilon})
\\
& = & \prod_{i=1}^n P(\mathbf{Y}_{\ast i}^{(T)}, \ldots, \mathbf{Y}_{\ast i}^{(1)})
\, \, \,  = \, \, \, \prod_{i=1}^n
\prod_{t=1}^T P(\mathbf{Y}_{\ast i}^{(t)} \, | \, \mathbf{Y}_{\ast i}^{(t-1)}, \ldots, \mathbf{Y}_{\ast i}^{(1)})
\\
& = & \prod_{i=1}^n \prod_{t=1}^T \frac{1}{(2\pi)^{p/2} | \SSigma_{\varepsilon} |^{1/2} } \exp \big[ - \big( \mathbf{Y}_{\ast i}^{(t)} - \nnu - \mathbf{A} \mathbf{Y}_{\ast i}^{(t-1)}  \big)^T \SSigma_{\varepsilon}^{-1} \big(\mathbf{Y}_{\ast i}^{(t)} - \nnu - \mathbf{A} \mathbf{Y}_{\ast i}^{(t-1)}  \big) \big]
\\
& = & \prod_{i=1}^n \prod_{t=1}^T \frac{1}{(2\pi)^{p/2} | \SSigma_{\varepsilon} |^{1/2} }  \exp \big\{ - \big[ \mathbf{Y}_{\ast i}^{(t)} - \nnu -
\big( \big( \mathbf{Y}_{\ast i}^{(t-1)} \big)^T  \otimes \mathbf{I}_{p \times p} \big) \mbox{vec}(\mathbf{A})  \big]^T
\\
& & \qquad \qquad \qquad \qquad  \qquad \qquad \qquad \, \SSigma_{\varepsilon}^{-1} \big[ \mathbf{Y}_{\ast i}^{(t)} - \nnu -
\big( \big( \mathbf{Y}_{\ast i}^{(t-1)} \big)^T  \otimes \mathbf{I}_{p \times p} \big) \mbox{vec}(\mathbf{A})  \big] \big\}
\end{eqnarray*}
% For the estimation of $\mathbf{A}$, we select only the part of the log-likelihood that involves $\mbox{vec}(\mathbf{A})$:
% \begin{eqnarray*}
% - \frac{1}{2}  \sum_{i=1}^n \sum_{t=1}^T  \big[ \mathbf{Y}_{\ast i}^{(t)} - \nnu -
% \big( \big( \mathbf{Y}_{\ast i}^{(t-1)} \big)^T  \otimes \mathbf{I}_{p \times p} \big) \mbox{vec}(\mathbf{A})  \big]^T
% \SSigma_{\varepsilon}^{-1} \big[ \mathbf{Y}_{\ast i}^{(t)} - \nnu -
% \big( \big( \mathbf{Y}_{\ast i}^{(t-1)} \big)^T  \otimes \mathbf{I}_{p \times p} \big) \mbox{vec}(\mathbf{A})  \big]
% \end{eqnarray*}
Equate the derivative of the log-likelihood with respect to $\mbox{vec}(\mathbf{A})$ to zero:
\begin{eqnarray*}
0 & = & \sum_{i=1}^n \sum_{t=1}^T \big( \big( \mathbf{Y}_{\ast i}^{(t-1)} \big)^T  \otimes \mathbf{I}_{p \times p} \big)^T  \SSigma_{\varepsilon}^{-1}
\big(\mathbf{Y}_{\ast i}^{(t)} - \nnu - \big( \big( \mathbf{Y}_{\ast i}^{(t-1)} \big)^T  \otimes \mathbf{I}_{p \times p} \big) \mbox{vec}(\mathbf{A}) \big).
% \\
% & = & \sum_{i=1}^n \sum_{t=1}^T \big( \mathbf{Y}_{\ast i}^{(t-1)}   \otimes \mathbf{I}_{p \times p} \big)  \SSigma_{\varepsilon}^{-1}
% \big(\mathbf{Y}_{\ast i}^{(t)} - \nnu - \big( \big( \mathbf{Y}_{\ast i}^{(t-1)} \big)^T  \otimes \mathbf{I}_{p \times p} \big) \mbox{vec}(\mathbf{A}) \big)
% \\
% & = & \sum_{i=1}^n \sum_{t=1}^T \big( \mathbf{Y}_{\ast i}^{(t-1)}   \otimes \mathbf{I}_{p \times p} \big) \big( 1 \otimes \SSigma_{\varepsilon}^{-1})
% \big(\mathbf{Y}_{\ast i}^{(t)} - \nnu - \big( \big( \mathbf{Y}_{\ast i}^{(t-1)} \big)^T  \otimes \mathbf{I}_{p \times p} \big) \mbox{vec}(\mathbf{A}) \big)
% \\
% & = & \sum_{i=1}^n \sum_{t=1}^T \big( \mathbf{Y}_{\ast i}^{(t-1)}  \otimes \SSigma_{\varepsilon}^{-1})
% \big(\mathbf{Y}_{\ast i}^{(t)} - \nnu - \big( \big( \mathbf{Y}_{\ast i}^{(t-1)} \big)^T  \otimes \mathbf{I}_{p \times p} \big) \mbox{vec}(\mathbf{A}) \big)
% \\
% & = & \sum_{i=1}^n \sum_{t=1}^T \big( \mathbf{Y}_{\ast i}^{(t-1)}  \otimes \SSigma_{\varepsilon}^{-1}\big)
% \big(\mathbf{Y}_{\ast i}^{(t)} - \nnu \big) -
% \sum_{i=1}^n \sum_{t=1}^T \big( \mathbf{Y}_{\ast i}^{(t-1)}  \otimes \SSigma_{\varepsilon}^{-1}\big) \big( \big( \mathbf{Y}_{\ast i}^{(t-1)} \big)^T  \otimes \mathbf{I}_{p \times p} \big) % \mbox{vec}(\mathbf{A})
% \\
% & = & \sum_{i=1}^n \sum_{t=1}^T \big( \mathbf{Y}_{\ast i}^{(t-1)}  \otimes \SSigma_{\varepsilon}^{-1}\big)
% \big(\mathbf{Y}_{\ast i}^{(t)} - \nnu \big) -
% \sum_{i=1}^n \sum_{t=1}^T \big( \mathbf{Y}_{\ast i}^{(t-1)} \big( \mathbf{Y}_{\ast i}^{(t-1)} \big)^T \otimes \SSigma_{\varepsilon}^{-1}\big) \mbox{vec}(\mathbf{A})
% \\
% & = & \big( 1 \otimes \SSigma_{\varepsilon}^{-1}\big) \sum_{i=1}^n \sum_{t=1}^T \big( \mathbf{Y}_{\ast i}^{(t-1)}  \otimes \mathbf{I}_{p \times p} \big)
% \big(\mathbf{Y}_{\ast i}^{(t)} - \nnu \big)
% \\
% & & \quad - \, \, \big( 1 \otimes \SSigma_{\varepsilon}^{-1}\big)  \sum_{i=1}^n \sum_{t=1}^T \big( \mathbf{Y}_{\ast i}^{(t-1)} \big( \mathbf{Y}_{\ast i}^{(t-1)} \big)^T \otimes \mathbf{I}_{p \times p} \big) \mbox{vec}(\mathbf{A})
% \\
% \big( 1 \otimes \SSigma_{\varepsilon}^{-1}\big) \sum_{i=1}^n \sum_{t=1}^T \big( \mathbf{Y}_{\ast i}^{(t-1)}  \otimes \mathbf{I}_{p \times p} \big)
% \big(\mathbf{Y}_{\ast i}^{(t)} - \nnu \big)
% & = & \big( 1 \otimes \SSigma_{\varepsilon}^{-1}\big)  \big( \sum_{i=1}^n \sum_{t=1}^T \mathbf{Y}_{\ast i}^{(t-1)} \big( \mathbf{Y}_{\ast i}^{(t-1)} \big)^T \otimes \mathbf{I}_{p \times p} \big) \mbox{vec}(\mathbf{A}).
\end{eqnarray*}
After some algebraic manipulations this is rewritten to:
\begin{eqnarray*}
0 & = & \sum_{i=1}^n \sum_{t=1}^T \big( \mathbf{Y}_{\ast i}^{(t-1)}  \otimes \SSigma_{\varepsilon}^{-1}\big)
\big(\mathbf{Y}_{\ast i}^{(t)} - \nnu \big) -
\sum_{i=1}^n \sum_{t=1}^T \big( \mathbf{Y}_{\ast i}^{(t-1)} \big( \mathbf{Y}_{\ast i}^{(t-1)} \big)^T \otimes \SSigma_{\varepsilon}^{-1}\big) \mbox{vec}(\mathbf{A}).
\end{eqnarray*}
Solve this estimation equation to obtain an estimate for $\mathbf{A}$:
\begin{eqnarray*}
\mbox{vec}(\hat{\mathbf{A}}) & = &
\Big(
\sum_{i=1}^n \sum_{t=1}^T \big( \mathbf{Y}_{\ast i}^{(t-1)} \big( \mathbf{Y}_{\ast i}^{(t-1)} \big)^T \otimes \mathbf{I}_{p \times p} \big)
\Big)^{-1}
\sum_{i=1}^n \sum_{t=1}^T \big( \mathbf{Y}_{\ast i}^{(t-1)}  \otimes \mathbf{I}_{p \times p} \big)
\big(\mathbf{Y}_{\ast i}^{(t)} - \nnu \big)
\end{eqnarray*}
Similarly, equate the derivative of the log-likelihood with respect to $\nnu$ to zero:
\begin{eqnarray*}
0 & = & \sum_{i=1}^n \sum_{t=1}^T \SSigma_{\varepsilon}^{-1}
\big(\mathbf{Y}_{\ast i}^{(t)} - \nnu - \big( \big( \mathbf{Y}_{\ast i}^{(t-1)} \big)^T  \otimes \mathbf{I}_{p \times p} \big) \mbox{vec}(\mathbf{A}) \big)
\end{eqnarray*}
Solve this estimation equation to obtain an estimate for $\nnu$:
\begin{eqnarray*}
\hat{\nnu} & = & \frac{1}{nT} \sum_{i=1}^n \sum_{t=2}^T
\big(\mathbf{Y}_{\ast i}^{(t)} - \big( \big( \mathbf{Y}_{\ast i}^{(t-1)} \big)^T  \otimes \mathbf{I}_{p \times p} \big) \mbox{vec}(\mathbf{A}) \big)
\\
& = & \frac{1}{nT} \sum_{i=1}^n \sum_{t=2}^T
\big(\mathbf{Y}_{\ast i}^{(t)} - \mathbf{A} \mathbf{Y}_{\ast i}^{(t-1)} \big)
\end{eqnarray*}
Similarly, for $\SSigma_{\varepsilon}$ we obtain:
\begin{eqnarray*}
\hat{\SSigma}_{\varepsilon} & = & \frac{1}{nT}  \sum_{i=1}^n \sum_{t=1}^T  \big[ \mathbf{Y}_{\ast i}^{(t)} - \nnu - \big( \big( \mathbf{Y}_{\ast i}^{(t-1)} \big)^T  \otimes \mathbf{I}_{p \times p} \big) \mbox{vec}(\mathbf{A})  \big] \big[ \mathbf{Y}_{\ast i}^{(t)} - \nnu - \big( \big( \mathbf{Y}_{\ast i}^{(t-1)} \big)^T  \otimes \mathbf{I}_{p \times p} \big) \mbox{vec}(\mathbf{A})  \big]^T.
\end{eqnarray*}
Thus, $\SSigma_{\varepsilon}$ is simply estimated by the sample residual covariance matrix.


\newpage
\subsection{Alternative formulation of $\mathbf{A}$ estimator}
In the above the estimator of $\mathbf{A}$ is derived and written as:
\begin{eqnarray*}
\mbox{vec}(\hat{\mathbf{A}}) & = &
\Big(
\sum_{i=1}^n \sum_{t=1}^T \big( \mathbf{Y}_{\ast i}^{(t-1)} \big( \mathbf{Y}_{\ast i}^{(t-1)} \big)^T \otimes \mathbf{I}_{p \times p} \big)
\Big)^{-1}
\sum_{i=1}^n \sum_{t=1}^T \big( \mathbf{Y}_{\ast i}^{(t-1)}  \otimes \mathbf{I}_{p \times p} \big)
\big(\mathbf{Y}_{\ast i}^{(t)} - \nnu \big)
\end{eqnarray*}
This expression can be reformulated.




\subsection{Estimation of mean and autocovariance}
The process mean is estimated by:
\begin{eqnarray*}
\hat{\mmu}_y & = & \frac{1}{nT} \sum_{i=1}^n \sum_{t=1}^T \mathbf{Y}_{\ast,i,t},
\end{eqnarray*}
which has the following expectation:
\begin{eqnarray*}
E(\hat{\mmu}_y) & = & \frac{1}{nT} \sum_{i=1}^n \sum_{t=1}^T E( \mathbf{Y}_{\ast,i,t} ) \, \, \, = \, \, \, \frac{1}{nT} \sum_{i=1}^n \sum_{t=1}^T \mmu_y
\, \, \, = \, \, \,  \mmu_y.
\end{eqnarray*}
Hence, the estimator for the mean is unbiased.

A natural estimator for the autocovariance (confer \citealt{Broc2006}) would be:
\begin{eqnarray*}
\hat{\mathbf{\Gamma}}(\tau) & = &
\left\{
\begin{array}{rcccl}
\frac{1}{n(T-\tau)} \sum_{i=1}^n \sum_{t=1}^{T-\tau} ( \mathbf{Y}_{\ast i}^{(t)} - \hat{\mmu}_y) \big(\mathbf{Y}_{\ast i}^{(t+\tau)} - \hat{\mmu}_y \big)^T & & \mbox{for} & 0 \leq \tau  \leq T -1,
\\
\frac{1}{n(T-\tau)} \sum_{i=1}^n \sum_{t=-\tau+1}^{T} ( \mathbf{Y}_{\ast i}^{(t)} - \hat{\mmu}_y) \big(\mathbf{Y}_{\ast i}^{(t+\tau)} - \hat{\mmu}_y \big)^T & &
\mbox{for} & -T+1 \leq \tau  < 0.
\end{array}
\right.
\end{eqnarray*}
For the calculation of the expectation of the covariance estimator we assume that $\mmu_y=0$ (as we have centered the gene expression levels around zero).
Then, for $\tau \geq 0$, we have:
\begin{eqnarray*}
E[ \hat{\mathbf{\Gamma}}(\tau) ] & = & \frac{1}{n(T-\tau)} \sum_{i=1}^n \sum_{t=1}^{T-\tau} E[( \mathbf{Y}_{\ast i}^{(t)} - \hat{\mmu}_y) \big(\mathbf{Y}_{\ast i}^{(t+\tau)} - \hat{\mmu}_y \big)^T ]
\\
& = & \frac{1}{n(T-\tau)} \sum_{i=1}^n \sum_{t=1}^{T-\tau} E[ \mathbf{Y}_{\ast i}^{(t)} \big(\mathbf{Y}_{\ast i}^{(t+\tau)}  \big)^T ] - E[ \mathbf{Y}_{\ast i}^{(t)} \hat{\mmu}_y^T ] - E[ \hat{\mmu}_y \big(\mathbf{Y}_{\ast i}^{(t+\tau)} \big)^T ] + E[\hat{\mmu}_y \hat{\mmu}_y^T ]
\\
& = &  \GGamma(\tau) + \frac{1}{n(T-\tau)} \sum_{i=1}^n \sum_{t=1}^{T-\tau} E[\hat{\mmu}_y \hat{\mmu}_y^T ] - E[ \mathbf{Y}_{\ast i}^{(t)} \hat{\mmu}_y^T ] - E[ \hat{\mmu}_y \big(\mathbf{Y}_{\ast i}^{(t+\tau)} \big)^T ].
\end{eqnarray*}
Furthermore, straightforward algebra learns:
\begin{eqnarray*}
& & \hspace{-1.5cm}  \frac{1}{n(T-\tau)} \sum_{i=1}^n \sum_{t=1}^{T-\tau} E[\hat{\mmu}_y \hat{\mmu}_y^T ]
\\
& = &  E[\hat{\mmu}_y \hat{\mmu}_y^T ] \, \, \, = \, \, \,  \frac{1}{n^2 T^2} \sum_{i_1=1}^n \sum_{i_2=1}^n \sum_{t_1=1}^{T} \sum_{t_2=1}^{T} E[\mathbf{Y}_{\ast i_1}^{(t_1)}  \big(\mathbf{Y}_{\ast i_2}^{(t_2)} \big)^T  ]
\\
&  = & \frac{1}{n^2 T^2} \sum_{i=1}^n \sum_{t_1=1}^{T} \sum_{t_2=1}^{T} E[\mathbf{Y}_{\ast i}^{(t_1)}  \big(\mathbf{Y}_{\ast i}^{(t_2)} \big)^T  ]
\, \, \, = \, \, \,  \frac{1}{n T^2} \sum_{t_1=1}^{T} \sum_{t_2=1}^{T} \GGamma(t_2 - t_1)
% \\
% & = &  \frac{1}{n T} \sum_{t=-T+1}^{T-1} \big(1 - \frac{|t|}{T} \big) \GGamma(t),
\end{eqnarray*}
and
\begin{eqnarray*}
\frac{1}{n(T-\tau)} \sum_{i=1}^n \sum_{t=1}^{T-\tau} E[ \mathbf{Y}_{\ast i}^{(t)} \hat{\mmu}_y^T ] & = &
\frac{1}{n^2 T (T-\tau)} \sum_{i_1=1}^n \sum_{i_2=1}^n \sum_{t_1=1}^{T-\tau} \sum_{t_2=1}^T E \big[ \mathbf{Y}_{\ast i_1}^{(t_1)} \big(\mathbf{Y}_{\ast i_2}^{(t_2)} \big)^T \big]
\\
& = & \frac{1}{n^2 T (T-\tau)} \sum_{i=1}^n \sum_{t_1=1}^{T-\tau} \sum_{t_2=1}^T E \big[ \mathbf{Y}_{\ast i}^{(t_1)} \big(\mathbf{Y}_{\ast i}^{(t_2)} \big)^T \big]
\\
& = & \frac{1}{n T (T-\tau)} \sum_{t_1=1}^{T-\tau} \sum_{t_2=1}^T \GGamma ( t_2 - t_1),
\end{eqnarray*}
and
\begin{eqnarray*}
\frac{1}{n(T-\tau)} \sum_{i=1}^n \sum_{t=1}^{T-\tau} E[ \hat{\mmu}_y  \big(\mathbf{Y}_{\ast i}^{(t+\tau)} \big)^T ] & = &
\frac{1}{n^2 T (T-\tau)} \sum_{i_1=1}^n \sum_{i_2=1}^n \sum_{t_1=1}^{T} \sum_{t_2=1}^{T-\tau} E \big[ \mathbf{Y}_{\ast i_1}^{(t_1)} \big(\mathbf{Y}_{\ast i_2}^{(t_2 + \tau)} \big)^T \big]
\\
& = & \frac{1}{n^2 T (T-\tau)} \sum_{i=1}^n \sum_{t_1=1}^{T} \sum_{t_2=1}^{T-\tau} E[ \mathbf{Y}_{\ast i}^{(t_1)} \big(\mathbf{Y}_{\ast i}^{(t_2+\tau)} \big)^T ]
\\
& = & \frac{1}{n T (T-\tau)} \sum_{t_1=1}^{T} \sum_{t_2=1}^{T-\tau} \GGamma ( t_2 - t_1).
\end{eqnarray*}
Summarizing:
\begin{eqnarray*}
E[ \hat{\mathbf{\Gamma}}(\tau) ] & = & \GGamma(\tau) +  \frac{1}{n T^2} \sum_{t_1=1}^{T} \sum_{t_2=1}^{T} \GGamma(t_2 - t_1) - \frac{1}{n T (T-\tau)} \sum_{t_1=1}^{T-\tau} \sum_{t_2=1}^T \GGamma ( t_2 - t_1) - \frac{1}{n T (T-\tau)} \sum_{t_1=1}^{T} \sum_{t_2=1}^{T-\tau} \GGamma ( t_2 - t_1)
\\
& = & \GGamma(\tau) +  \frac{1}{n T^2} \sum_{t_1=1}^{T} \sum_{t_2=1}^{T} \GGamma(t_2 - t_1) - \frac{1}{n T (T-\tau)} \sum_{t_1=1}^T \sum_{t_2=1}^{T-\tau} \GGamma ( t_1 - t_2) - \frac{1}{n T (T-\tau)} \sum_{t_1=1}^{T} \sum_{t_2=1}^{T-\tau} \GGamma ( t_2 - t_1)
\\
\\
& = & \GGamma(\tau) +  \frac{1}{n T} \sum_{t_1=1}^{T} \left[ \frac{1}{T}\sum_{t_2=1}^{T} \GGamma(t_2 - t_1) - \frac{1}{T-\tau}  \sum_{t_2=1}^{T-\tau} \GGamma ( t_1 - t_2) - \frac{1}{T-\tau} \sum_{t_2=1}^{T-\tau} \GGamma ( t_2 - t_1) \right].
\end{eqnarray*}
This is not unbiased.



\end{document}



\begin{eqnarray*}
\mathcal{L}(\mathbf{Y}; \nnu, \mathbf{A}, \SSigma_{\varepsilon}) & \propto & \frac{1}{2}  \sum_{i=1}^n \sum_{t=1}^T  \log( | \SSigma_{\varepsilon} |)  -  \big[ \big( \mathbf{Y}_{\ast i}^{(t)} -  \nnu - \mathbf{A} \mathbf{Y}_{\ast i}^{(t-1)}  \big)^T \SSigma_{\varepsilon}^{-1} \big(\mathbf{Y}_{\ast i}^{(t)} - \nnu - \mathbf{A} \mathbf{Y}_{\ast i}^{(t-1)} \big) \big].
\end{eqnarray*}



On the convergence of the series. Replace $\mathbf{\Sigma}_{\varepsilon}$ by its eigen-decomposition $\mathbf{Q}_{\varepsilon} \LLambda_{\varepsilon} \mathbf{Q}_{\varepsilon}^T$
and apply the change-of-variables $\mathbf{\Sigma}_{y} = \mathbf{Q}_{\varepsilon} \LLambda_{\varepsilon}^{1/2} \tilde{\mathbf{\Sigma}}_{y} \LLambda_{\varepsilon}^{1/2} \mathbf{Q}_{\varepsilon}^T$:
\begin{eqnarray*}
\mathbf{Q}_{\varepsilon} \LLambda_{\varepsilon}^{1/2} \tilde{\mathbf{\Sigma}}_{y} \LLambda_{\varepsilon}^{1/2} \mathbf{Q}_{\varepsilon}^T & = & \mathbf{A} \mathbf{Q}_{\varepsilon} \LLambda_{\varepsilon}^{1/2} \tilde{\mathbf{\Sigma}}_{y} \LLambda_{\varepsilon}^{1/2} \mathbf{Q}_{\varepsilon}^T \mathbf{A}^T + \mathbf{Q}_{\varepsilon} \LLambda_{\varepsilon} \mathbf{Q}_{\varepsilon}^T
\end{eqnarray*}
and pre- and post multiplying by $\LLambda_{\varepsilon}^{-1/2} \mathbf{Q}_{\varepsilon}^T$ and $\mathbf{Q}_{\varepsilon} \LLambda_{\varepsilon}^{-1/2}$, respectively:
\begin{eqnarray*}
\tilde{\mathbf{\Sigma}}_{y}  & = & \LLambda_{\varepsilon}^{-1/2} \mathbf{Q}_{\varepsilon}^T \mathbf{A} \mathbf{Q}_{\varepsilon}  \LLambda_{\varepsilon}^{1/2}  \tilde{\mathbf{\Sigma}}_{y}  \LLambda_{\varepsilon}^{1/2} \mathbf{Q}_{\varepsilon}^T \mathbf{A}^T \mathbf{Q}_{\varepsilon} \LLambda_{\varepsilon}^{-1/2} + \mathbf{I}_{p \times p}
\\
& = & \tilde{\mathbf{A}} \tilde{\mathbf{\Sigma}}_{y} \tilde{\mathbf{A}}^T + \mathbf{I}_{p \times p}
\end{eqnarray*}
where $\tilde{\mathbf{A}} = \LLambda_{\varepsilon}^{-1/2} \mathbf{Q}_{\varepsilon}^T \mathbf{A} \mathbf{Q}_{\varepsilon}  \LLambda_{\varepsilon}^{1/2}$.

% \begin{eqnarray*}
% \mathbf{V} \tilde{\mathbf{\Sigma}}_{y} \mathbf{V}^T & = & \sum_{k=0}^{\infty}  \mathbf{D}^k \mathbf{V} \mathbf{V}^T \mathbf{D}^k
% \end{eqnarray*}
% Furthermore note that the square root of the eigenvalues of $\mathbf{X} \mathbf{X}^T$ and $\mathbf{X}^T \mathbf{X}$ equal the singular values of $\mathbf{X}$ (as obtained by singular values decomposition).

% check: http://math.stackexchange.com/questions/21055/a-geometric-infinite-sum-of-matrices


Or, when $\LLambda = \lambda \mathbf{I}_{2p \times 2p}$:
\begin{eqnarray*}
\mbox{vec}(\hat{\mathbf{A}}(\lambda )) & = &
\Big(\Big(\lambda \mathbf{I}_{p \times p} + \sum_{i=1}^n \sum_{t=1}^T \mathbf{Y}_{\ast i}^{(t-1)} \big( \mathbf{Y}_{\ast i}^{(t-1)} \big)^T\Big)^{-1} \otimes \mathbf{I}_{p \times p}
\Big) \sum_{i=1}^n \sum_{t=1}^T \big( \mathbf{Y}_{\ast i}^{(t-1)}  \otimes \mathbf{I}_{p \times p} \big)
\big(\mathbf{Y}_{\ast i}^{(t)} - \nnu \big).
\end{eqnarray*}
In any case, ridge estimation in the VAR(1) model is fully analogous to that in the linear regression model.


% \begin{eqnarray*}
% \left(
% \begin{array}{c}
% \mathbf{Y}_{\ast, i, t}
% \\
% \mathbf{Z}_{\ast, i, t}
% \end{array}
% \right)
% & = &
% \left(
% \begin{array}{cc}
% \mathbf{A}  & \mathbf{0}_{p \times p}
% \\
% \mathbf{0}_{p \times p} & \mathbf{A}
% \end{array}
% \right)
% \left(
% \begin{array}{c}
% \mathbf{Y}_{\ast, i, t-1}
% \\
% \mathbf{Y}_{\ast, i, t-1}
% \end{array}
% \right)
% +
% \left(
% \begin{array}{cc}
% \mathbf{I}_{p \times p} & \mathbf{0}_{p \times p}
% \\
% \mathbf{0}_{p \times p} & \mathbf{I}_{p \times p}
% \end{array}
% \right)
% \left(
% \begin{array}{c}
%  \vvarepsilon_{\ast, i, t}
% \\
%  \vvarepsilon_{\ast, i, t}
% \end{array}
% \right)
% +
% \left(
% \begin{array}{c}
% \mathbf{0}_{p \times 1}
% \\
% \mathbf{u}_{\ast, i, t}
% \end{array}
% \right),
% \end{eqnarray*}


Rewrite the inverse of the right-hand side: $\tilde{\SSigma}_y(\lambda) = \lambda \mathbf{I}_{p \times p} + \SSigma_y$. Substitute this ($\SSigma_y = \tilde{\SSigma}_y(\lambda) - \lambda \mathbf{I}_{p \times p}$) into the discrete Lyapunov equation. Thus:
\begin{eqnarray*}
\mathbf{\Sigma}_{y} & = & \mathbf{A} \mathbf{\Sigma}_{y} \mathbf{A}^{\mathrm{T}} + \mathbf{\Sigma}_{\varepsilon}
\end{eqnarray*}
now gives:
\begin{eqnarray*}
\tilde{\mathbf{\Sigma}}_{y}(\lambda) - \lambda \mathbf{I}_{p \times p} & = & \mathbf{A} \tilde{\mathbf{\Sigma}}_{y}(\lambda) \mathbf{A}^{\mathrm{T}} -
\lambda \mathbf{A} \mathbf{A}^{\mathrm{T}} + \mathbf{\Sigma}_{\varepsilon}.
\end{eqnarray*}
Or, rewritten:
\begin{eqnarray*}
\tilde{\mathbf{\Sigma}}_{y}(\lambda)  & = & \mathbf{A} \tilde{\mathbf{\Sigma}}_{y}(\lambda) \mathbf{A}^{\mathrm{T}} + \mathbf{\Sigma}_{\varepsilon} + \lambda \mathbf{I}_{p \times p} -
\lambda \mathbf{A} \mathbf{A}^{\mathrm{T}}.
\end{eqnarray*}
For this equation, we can write down the series solution for $\tilde{\mathbf{\Sigma}}_{y}(\lambda)$:
\begin{eqnarray*}
\tilde{\mathbf{\Sigma}}_{y}(\lambda) & = & \sum_{k=0}^{\infty} \mathbf{A}^k [ \mathbf{\Sigma}_{\varepsilon} + \lambda \mathbf{I}_{p \times p} -
\lambda \mathbf{A} \mathbf{A}^{\mathrm{T}}
] (\mathbf{A}^{\mathrm{T}})^k
\\
& = & \sum_{k=0}^{\infty} \mathbf{A}^k \mathbf{\Sigma}_{\varepsilon} (\mathbf{A}^{\mathrm{T}})^k
 +
\lambda \sum_{k=0}^{\infty} \mathbf{A}^k (\mathbf{A}^{\mathrm{T}})^k
-
\lambda \sum_{k=0}^{\infty} \mathbf{A}^{k+1} (\mathbf{A}^{\mathrm{T}})^{k+1}
\\
& = & \SSigma_y + \lambda \mathbf{I}_{p \times p}.
\end{eqnarray*}
Clearly, it also satisfies the discrete Lyapunov equation, but with a different choice of $\SSigma_{\varepsilon}$, i.e. $\tilde{\SSigma}_{\varepsilon} (\lambda) = \mathbf{\Sigma}_{\varepsilon} + \lambda \mathbf{I}_{p \times p} - \lambda \mathbf{A} \mathbf{A}^{\mathrm{T}}$. \reminder{Hmm, this need not define a proper covariance matrix.}


% If we define a new VAR(1) process with identical parameters as the original process and $\mathbf{u}_{\ast, i, t} \sim \mathcal{N}(\mathbf{0}_{p \times 1}, \lambda \mathbf{I}_{p \times p})$:
% \begin{eqnarray*}
% \left(
% \begin{array}{c}
% \mathbf{Y}_{\ast, i, t}
% \\
% \mathbf{Z}_{\ast, i, t}
% \end{array}
% \right)
% & = &
% \left(
% \begin{array}{cc}
% \mathbf{A}  & \mathbf{0}_{p \times p}
% \\
% \mathbf{0}_{p \times p} & \mathbf{I}_{p \times p}
% \end{array}
% \right)
% \left(
% \begin{array}{c}
% \mathbf{Y}_{\ast, i, t-1}
% \\
% \mathbf{0}_{p \times 1}
% \end{array}
% \right)
% +
% \left(
% \begin{array}{cc}
% \mathbf{I}_{p \times p} & \mathbf{0}_{p \times p}
% \\
% \mathbf{0}_{p \times p} & \mathbf{A}
% \end{array}
% \right)
% \left(
% \begin{array}{c}
% \mathbf{u}_{\ast, i, t}
% \\
% \mathbf{u}_{\ast, i, t}
% \end{array}
% \right)
% +
% \left(
% \begin{array}{c}
% \vvarepsilon_{\ast, i, t}
% \\
% \mathbf{0}_{p \times 1}
% \end{array}
% \right),
% \end{eqnarray*}



\begin{eqnarray*}
& & \hspace{-1.5cm} P(\mathbf{Y}_{\ast, i, t}, \mathbf{Y}_{\ast, i, t-1}, \ldots)
\\
& = & P(\mathbf{Y}_{\ast, i, t} \, | \, \mathbf{Y}_{\ast, i, t-1}) \, \cdot \, P(\mathbf{Y}_{\ast, i, t-1} \, | \, \mathbf{Y}_{\ast, i, t-2}) \, \cdot \, \ldots
\\
& \propto & \exp \left[ -(\mathbf{Y}_{\ast, i, t}  - \mathbf{A} \mathbf{Y}_{\ast, i, t-1})^{\mathrm{T}} \, \SSigma_{\varepsilon}^{-1}
\, (\mathbf{Y}_{\ast, i, t}  - \mathbf{A} \mathbf{Y}_{\ast, i, t-1}) / 2 \right] \, \cdot \, \ldots
\\
& = & \exp \Big[ -\frac{1}{2} \sum_{j_1, j_2=1}^p (\mathbf{Y}_{\ast, i, t}  - \mathbf{A} \mathbf{Y}_{\ast, i, t-1})_{j_1, 1} \,  (\SSigma_{\varepsilon}^{-1})_{j_1, j_2} \, (\mathbf{Y}_{\ast, i, t}  - \mathbf{A} \mathbf{Y}_{\ast, i, t-1})_{j_2, 1} \Big] \, \cdot \, \ldots
\\
& = & \exp \Big[ -\frac{1}{2} \sum_{j_1, j_2=1}^p
(\SSigma_{\varepsilon}^{-1})_{j_1, j_2}  Y_{j_1, i, t}
Y_{j_2, i, t} + (\SSigma_{\varepsilon}^{-1})_{j_1, j_2}
(\mathbf{A} \mathbf{Y}_{\ast, i, t-1})_{j_1, 1}
(\mathbf{A} \mathbf{Y}_{\ast, i, t-1})_{j_2, 1}
\\
&   & \qquad \qquad
- (\SSigma_{\varepsilon}^{-1})_{j_1, j_2}
\mathbf{Y}_{j_1, i, t}
(\mathbf{A} \mathbf{Y}_{\ast, i, t-1})_{j_2, 1}
- (\SSigma_{\varepsilon}^{-1})_{j_1, j_2}
\mathbf{Y}_{j_2, i, t}
(\mathbf{A} \mathbf{Y}_{\ast, i, t-1})_{j_1, 1} \Big] \, \cdot \, \ldots
\end{eqnarray*}
